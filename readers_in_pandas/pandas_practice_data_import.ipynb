{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652a18db",
   "metadata": {},
   "source": [
    "# pandas Data Import best Practices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d02df59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pandas\n",
      "Version: 2.2.3\n",
      "Summary: Powerful data structures for data analysis, time series, and statistics\n",
      "Home-page: https://pandas.pydata.org\n",
      "Author: \n",
      "Author-email: The Pandas Development Team <pandas-dev@python.org>\n",
      "License: BSD 3-Clause License\n",
      "\n",
      "Copyright (c) 2008-2011, AQR Capital Management, LLC, Lambda Foundry, Inc. and PyData Development Team\n",
      "All rights reserved.\n",
      "\n",
      "Copyright (c) 2011-2023, Open source contributors.\n",
      "\n",
      "Redistribution and use in source and binary forms, with or without\n",
      "modification, are permitted provided that the following conditions are met:\n",
      "\n",
      "* Redistributions of source code must retain the above copyright notice, this\n",
      "  list of conditions and the following disclaimer.\n",
      "\n",
      "* Redistributions in binary form must reproduce the above copyright notice,\n",
      "  this list of conditions and the following disclaimer in the documentation\n",
      "  and/or other materials provided with the distribution.\n",
      "\n",
      "* Neither the name of the copyright holder nor the names of its\n",
      "  contributors may be used to endorse or promote products derived from\n",
      "  this software without specific prior written permission.\n",
      "\n",
      "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "Copyright (c) 2010-2019 Keith Goodman\n",
      "Copyright (c) 2019 Bottleneck Developers\n",
      "All rights reserved.\n",
      "\n",
      "Redistribution and use in source and binary forms, with or without\n",
      "modification, are permitted provided that the following conditions are met:\n",
      "\n",
      "    * Redistributions of source code must retain the above copyright notice,\n",
      "      this list of conditions and the following disclaimer.\n",
      "\n",
      "    * Redistributions in binary form must reproduce the above copyright\n",
      "      notice, this list of conditions and the following disclaimer in the\n",
      "      documentation and/or other materials provided with the distribution.\n",
      "\n",
      "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n",
      "ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n",
      "LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n",
      "CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n",
      "SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n",
      "INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n",
      "CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n",
      "ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n",
      "POSSIBILITY OF SUCH DAMAGE.Copyright 2017- Paul Ganssle <paul@ganssle.io>\n",
      "Copyright 2017- dateutil contributors (see AUTHORS file)\n",
      "\n",
      "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "   you may not use this file except in compliance with the License.\n",
      "   You may obtain a copy of the License at\n",
      "\n",
      "       http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "   Unless required by applicable law or agreed to in writing, software\n",
      "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "   See the License for the specific language governing permissions and\n",
      "   limitations under the License.\n",
      "\n",
      "The above license applies to all contributions after 2017-12-01, as well as\n",
      "all contributions that have been re-licensed (see AUTHORS file for the list of\n",
      "contributors who have re-licensed their code).\n",
      "--------------------------------------------------------------------------------\n",
      "dateutil - Extensions to the standard Python datetime module.\n",
      "\n",
      "Copyright (c) 2003-2011 - Gustavo Niemeyer <gustavo@niemeyer.net>\n",
      "Copyright (c) 2012-2014 - Tomi Pievil√§inen <tomi.pievilainen@iki.fi>\n",
      "Copyright (c) 2014-2016 - Yaron de Leeuw <me@jarondl.net>\n",
      "Copyright (c) 2015-     - Paul Ganssle <paul@ganssle.io>\n",
      "Copyright (c) 2015-     - dateutil contributors (see AUTHORS file)\n",
      "\n",
      "All rights reserved.\n",
      "\n",
      "Redistribution and use in source and binary forms, with or without\n",
      "modification, are permitted provided that the following conditions are met:\n",
      "\n",
      "    * Redistributions of source code must retain the above copyright notice,\n",
      "      this list of conditions and the following disclaimer.\n",
      "    * Redistributions in binary form must reproduce the above copyright notice,\n",
      "      this list of conditions and the following disclaimer in the documentation\n",
      "      and/or other materials provided with the distribution.\n",
      "    * Neither the name of the copyright holder nor the names of its\n",
      "      contributors may be used to endorse or promote products derived from\n",
      "      this software without specific prior written permission.\n",
      "\n",
      "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n",
      "\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n",
      "LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n",
      "A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n",
      "CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n",
      "EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n",
      "PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n",
      "PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n",
      "LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n",
      "NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n",
      "SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "\n",
      "The above BSD License Applies to all code, even that also covered by Apache 2.0.# MIT License\n",
      "\n",
      "Copyright (c) 2019 Hadley Wickham; RStudio; and Evan Miller\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "of this software and associated documentation files (the \"Software\"), to deal\n",
      "in the Software without restriction, including without limitation the rights\n",
      "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "copies of the Software, and to permit persons to whom the Software is\n",
      "furnished to do so, subject to the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be included in all\n",
      "copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "SOFTWARE.\n",
      "Based on http://opensource.org/licenses/MIT\n",
      "\n",
      "This is a template. Complete and ship as file LICENSE the following 2\n",
      "lines (only)\n",
      "\n",
      "YEAR:\n",
      "COPYRIGHT HOLDER:\n",
      "\n",
      "and specify as\n",
      "\n",
      "License: MIT + file LICENSE\n",
      "\n",
      "Copyright (c) <YEAR>, <COPYRIGHT HOLDER>\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining\n",
      "a copy of this software and associated documentation files (the\n",
      "\"Software\"), to deal in the Software without restriction, including\n",
      "without limitation the rights to use, copy, modify, merge, publish,\n",
      "distribute, sublicense, and/or sell copies of the Software, and to\n",
      "permit persons to whom the Software is furnished to do so, subject to\n",
      "the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be\n",
      "included in all copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n",
      "EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
      "MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n",
      "NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\n",
      "LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n",
      "OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n",
      "WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
      "The MIT License\n",
      "\n",
      "Copyright (c) 2008-     Attractive Chaos <attractor@live.co.uk>\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining\n",
      "a copy of this software and associated documentation files (the\n",
      "\"Software\"), to deal in the Software without restriction, including\n",
      "without limitation the rights to use, copy, modify, merge, publish,\n",
      "distribute, sublicense, and/or sell copies of the Software, and to\n",
      "permit persons to whom the Software is furnished to do so, subject to\n",
      "the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be\n",
      "included in all copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n",
      "EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
      "MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n",
      "NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n",
      "BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n",
      "ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
      "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "SOFTWARE.musl as a whole is licensed under the following standard MIT license:\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Copyright ¬© 2005-2020 Rich Felker, et al.\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining\n",
      "a copy of this software and associated documentation files (the\n",
      "\"Software\"), to deal in the Software without restriction, including\n",
      "without limitation the rights to use, copy, modify, merge, publish,\n",
      "distribute, sublicense, and/or sell copies of the Software, and to\n",
      "permit persons to whom the Software is furnished to do so, subject to\n",
      "the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be\n",
      "included in all copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n",
      "EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
      "MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n",
      "IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n",
      "CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n",
      "TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n",
      "SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Authors/contributors include:\n",
      "\n",
      "A. Wilcox\n",
      "Ada Worcester\n",
      "Alex Dowad\n",
      "Alex Suykov\n",
      "Alexander Monakov\n",
      "Andre McCurdy\n",
      "Andrew Kelley\n",
      "Anthony G. Basile\n",
      "Aric Belsito\n",
      "Arvid Picciani\n",
      "Bartosz Brachaczek\n",
      "Benjamin Peterson\n",
      "Bobby Bingham\n",
      "Boris Brezillon\n",
      "Brent Cook\n",
      "Chris Spiegel\n",
      "Cl√©ment Vasseur\n",
      "Daniel Micay\n",
      "Daniel Sabogal\n",
      "Daurnimator\n",
      "David Carlier\n",
      "David Edelsohn\n",
      "Denys Vlasenko\n",
      "Dmitry Ivanov\n",
      "Dmitry V. Levin\n",
      "Drew DeVault\n",
      "Emil Renner Berthing\n",
      "Fangrui Song\n",
      "Felix Fietkau\n",
      "Felix Janda\n",
      "Gianluca Anzolin\n",
      "Hauke Mehrtens\n",
      "He X\n",
      "Hiltjo Posthuma\n",
      "Isaac Dunham\n",
      "Jaydeep Patil\n",
      "Jens Gustedt\n",
      "Jeremy Huntwork\n",
      "Jo-Philipp Wich\n",
      "Joakim Sindholt\n",
      "John Spencer\n",
      "Julien Ramseier\n",
      "Justin Cormack\n",
      "Kaarle Ritvanen\n",
      "Khem Raj\n",
      "Kylie McClain\n",
      "Leah Neukirchen\n",
      "Luca Barbato\n",
      "Luka Perkov\n",
      "M Farkas-Dyck (Strake)\n",
      "Mahesh Bodapati\n",
      "Markus Wichmann\n",
      "Masanori Ogino\n",
      "Michael Clark\n",
      "Michael Forney\n",
      "Mikhail Kremnyov\n",
      "Natanael Copa\n",
      "Nicholas J. Kain\n",
      "orc\n",
      "Pascal Cuoq\n",
      "Patrick Oppenlander\n",
      "Petr Hosek\n",
      "Petr Skocik\n",
      "Pierre Carrier\n",
      "Reini Urban\n",
      "Rich Felker\n",
      "Richard Pennington\n",
      "Ryan Fairfax\n",
      "Samuel Holland\n",
      "Segev Finer\n",
      "Shiz\n",
      "sin\n",
      "Solar Designer\n",
      "Stefan Kristiansson\n",
      "Stefan O'Rear\n",
      "Szabolcs Nagy\n",
      "Timo Ter√§s\n",
      "Trutz Behn\n",
      "Valentin Ochs\n",
      "Will Dietz\n",
      "William Haddon\n",
      "William Pitcock\n",
      "\n",
      "Portions of this software are derived from third-party works licensed\n",
      "under terms compatible with the above MIT license:\n",
      "\n",
      "The TRE regular expression implementation (src/regex/reg* and\n",
      "src/regex/tre*) is Copyright ¬© 2001-2008 Ville Laurikari and licensed\n",
      "under a 2-clause BSD license (license text in the source files). The\n",
      "included version has been heavily modified by Rich Felker in 2012, in\n",
      "the interests of size, simplicity, and namespace cleanliness.\n",
      "\n",
      "Much of the math library code (src/math/* and src/complex/*) is\n",
      "Copyright ¬© 1993,2004 Sun Microsystems or\n",
      "Copyright ¬© 2003-2011 David Schultz or\n",
      "Copyright ¬© 2003-2009 Steven G. Kargl or\n",
      "Copyright ¬© 2003-2009 Bruce D. Evans or\n",
      "Copyright ¬© 2008 Stephen L. Moshier or\n",
      "Copyright ¬© 2017-2018 Arm Limited\n",
      "and labelled as such in comments in the individual source files. All\n",
      "have been licensed under extremely permissive terms.\n",
      "\n",
      "The ARM memcpy code (src/string/arm/memcpy.S) is Copyright ¬© 2008\n",
      "The Android Open Source Project and is licensed under a two-clause BSD\n",
      "license. It was taken from Bionic libc, used on Android.\n",
      "\n",
      "The AArch64 memcpy and memset code (src/string/aarch64/*) are\n",
      "Copyright ¬© 1999-2019, Arm Limited.\n",
      "\n",
      "The implementation of DES for crypt (src/crypt/crypt_des.c) is\n",
      "Copyright ¬© 1994 David Burren. It is licensed under a BSD license.\n",
      "\n",
      "The implementation of blowfish crypt (src/crypt/crypt_blowfish.c) was\n",
      "originally written by Solar Designer and placed into the public\n",
      "domain. The code also comes with a fallback permissive license for use\n",
      "in jurisdictions that may not recognize the public domain.\n",
      "\n",
      "The smoothsort implementation (src/stdlib/qsort.c) is Copyright ¬© 2011\n",
      "Valentin Ochs and is licensed under an MIT-style license.\n",
      "\n",
      "The x86_64 port was written by Nicholas J. Kain and is licensed under\n",
      "the standard MIT terms.\n",
      "\n",
      "The mips and microblaze ports were originally written by Richard\n",
      "Pennington for use in the ellcc project. The original code was adapted\n",
      "by Rich Felker for build system and code conventions during upstream\n",
      "integration. It is licensed under the standard MIT terms.\n",
      "\n",
      "The mips64 port was contributed by Imagination Technologies and is\n",
      "licensed under the standard MIT terms.\n",
      "\n",
      "The powerpc port was also originally written by Richard Pennington,\n",
      "and later supplemented and integrated by John Spencer. It is licensed\n",
      "under the standard MIT terms.\n",
      "\n",
      "All other files which have no copyright comments are original works\n",
      "produced specifically for use as part of this library, written either\n",
      "by Rich Felker, the main author of the library, or by one or more\n",
      "contibutors listed above. Details on authorship of individual files\n",
      "can be found in the git version control history of the project. The\n",
      "omission of copyright and license comments in each file is in the\n",
      "interest of source tree size.\n",
      "\n",
      "In addition, permission is hereby granted for all public header files\n",
      "(include/* and arch/*/bits/*) and crt files intended to be linked into\n",
      "applications (crt/*, ldso/dlstart.c, and arch/*/crt_arch.h) to omit\n",
      "the copyright notice and permission notice otherwise required by the\n",
      "license, and to use these files without any requirement of\n",
      "attribution. These files include substantial contributions from:\n",
      "\n",
      "Bobby Bingham\n",
      "John Spencer\n",
      "Nicholas J. Kain\n",
      "Rich Felker\n",
      "Richard Pennington\n",
      "Stefan Kristiansson\n",
      "Szabolcs Nagy\n",
      "\n",
      "all of whom have explicitly granted such permission.\n",
      "\n",
      "This file previously contained text expressing a belief that most of\n",
      "the files covered by the above exception were sufficiently trivial not\n",
      "to be subject to copyright, resulting in confusion over whether it\n",
      "negated the permissions granted in the license. In the spirit of\n",
      "permissive licensing, and of not having licensing issues being an\n",
      "obstacle to adoption, that text has been removed.Copyright (c) 2005-2023, NumPy Developers.\n",
      "All rights reserved.\n",
      "\n",
      "Redistribution and use in source and binary forms, with or without\n",
      "modification, are permitted provided that the following conditions are\n",
      "met:\n",
      "\n",
      "    * Redistributions of source code must retain the above copyright\n",
      "       notice, this list of conditions and the following disclaimer.\n",
      "\n",
      "    * Redistributions in binary form must reproduce the above\n",
      "       copyright notice, this list of conditions and the following\n",
      "       disclaimer in the documentation and/or other materials provided\n",
      "       with the distribution.\n",
      "\n",
      "    * Neither the name of the NumPy Developers nor the names of any\n",
      "       contributors may be used to endorse or promote products derived\n",
      "       from this software without specific prior written permission.\n",
      "\n",
      "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n",
      "\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n",
      "LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n",
      "A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n",
      "OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n",
      "SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n",
      "LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n",
      "DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n",
      "THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
      "(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "                                 Apache License\n",
      "                           Version 2.0, January 2004\n",
      "                        http://www.apache.org/licenses/\n",
      "\n",
      "   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
      "\n",
      "   1. Definitions.\n",
      "\n",
      "      \"License\" shall mean the terms and conditions for use, reproduction,\n",
      "      and distribution as defined by Sections 1 through 9 of this document.\n",
      "\n",
      "      \"Licensor\" shall mean the copyright owner or entity authorized by\n",
      "      the copyright owner that is granting the License.\n",
      "\n",
      "      \"Legal Entity\" shall mean the union of the acting entity and all\n",
      "      other entities that control, are controlled by, or are under common\n",
      "      control with that entity. For the purposes of this definition,\n",
      "      \"control\" means (i) the power, direct or indirect, to cause the\n",
      "      direction or management of such entity, whether by contract or\n",
      "      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n",
      "      outstanding shares, or (iii) beneficial ownership of such entity.\n",
      "\n",
      "      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n",
      "      exercising permissions granted by this License.\n",
      "\n",
      "      \"Source\" form shall mean the preferred form for making modifications,\n",
      "      including but not limited to software source code, documentation\n",
      "      source, and configuration files.\n",
      "\n",
      "      \"Object\" form shall mean any form resulting from mechanical\n",
      "      transformation or translation of a Source form, including but\n",
      "      not limited to compiled object code, generated documentation,\n",
      "      and conversions to other media types.\n",
      "\n",
      "      \"Work\" shall mean the work of authorship, whether in Source or\n",
      "      Object form, made available under the License, as indicated by a\n",
      "      copyright notice that is included in or attached to the work\n",
      "      (an example is provided in the Appendix below).\n",
      "\n",
      "      \"Derivative Works\" shall mean any work, whether in Source or Object\n",
      "      form, that is based on (or derived from) the Work and for which the\n",
      "      editorial revisions, annotations, elaborations, or other modifications\n",
      "      represent, as a whole, an original work of authorship. For the purposes\n",
      "      of this License, Derivative Works shall not include works that remain\n",
      "      separable from, or merely link (or bind by name) to the interfaces of,\n",
      "      the Work and Derivative Works thereof.\n",
      "\n",
      "      \"Contribution\" shall mean any work of authorship, including\n",
      "      the original version of the Work and any modifications or additions\n",
      "      to that Work or Derivative Works thereof, that is intentionally\n",
      "      submitted to Licensor for inclusion in the Work by the copyright owner\n",
      "      or by an individual or Legal Entity authorized to submit on behalf of\n",
      "      the copyright owner. For the purposes of this definition, \"submitted\"\n",
      "      means any form of electronic, verbal, or written communication sent\n",
      "      to the Licensor or its representatives, including but not limited to\n",
      "      communication on electronic mailing lists, source code control systems,\n",
      "      and issue tracking systems that are managed by, or on behalf of, the\n",
      "      Licensor for the purpose of discussing and improving the Work, but\n",
      "      excluding communication that is conspicuously marked or otherwise\n",
      "      designated in writing by the copyright owner as \"Not a Contribution.\"\n",
      "\n",
      "      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n",
      "      on behalf of whom a Contribution has been received by Licensor and\n",
      "      subsequently incorporated within the Work.\n",
      "\n",
      "   2. Grant of Copyright License. Subject to the terms and conditions of\n",
      "      this License, each Contributor hereby grants to You a perpetual,\n",
      "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
      "      copyright license to reproduce, prepare Derivative Works of,\n",
      "      publicly display, publicly perform, sublicense, and distribute the\n",
      "      Work and such Derivative Works in Source or Object form.\n",
      "\n",
      "   3. Grant of Patent License. Subject to the terms and conditions of\n",
      "      this License, each Contributor hereby grants to You a perpetual,\n",
      "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
      "      (except as stated in this section) patent license to make, have made,\n",
      "      use, offer to sell, sell, import, and otherwise transfer the Work,\n",
      "      where such license applies only to those patent claims licensable\n",
      "      by such Contributor that are necessarily infringed by their\n",
      "      Contribution(s) alone or by combination of their Contribution(s)\n",
      "      with the Work to which such Contribution(s) was submitted. If You\n",
      "      institute patent litigation against any entity (including a\n",
      "      cross-claim or counterclaim in a lawsuit) alleging that the Work\n",
      "      or a Contribution incorporated within the Work constitutes direct\n",
      "      or contributory patent infringement, then any patent licenses\n",
      "      granted to You under this License for that Work shall terminate\n",
      "      as of the date such litigation is filed.\n",
      "\n",
      "   4. Redistribution. You may reproduce and distribute copies of the\n",
      "      Work or Derivative Works thereof in any medium, with or without\n",
      "      modifications, and in Source or Object form, provided that You\n",
      "      meet the following conditions:\n",
      "\n",
      "      (a) You must give any other recipients of the Work or\n",
      "          Derivative Works a copy of this License; and\n",
      "\n",
      "      (b) You must cause any modified files to carry prominent notices\n",
      "          stating that You changed the files; and\n",
      "\n",
      "      (c) You must retain, in the Source form of any Derivative Works\n",
      "          that You distribute, all copyright, patent, trademark, and\n",
      "          attribution notices from the Source form of the Work,\n",
      "          excluding those notices that do not pertain to any part of\n",
      "          the Derivative Works; and\n",
      "\n",
      "      (d) If the Work includes a \"NOTICE\" text file as part of its\n",
      "          distribution, then any Derivative Works that You distribute must\n",
      "          include a readable copy of the attribution notices contained\n",
      "          within such NOTICE file, excluding those notices that do not\n",
      "          pertain to any part of the Derivative Works, in at least one\n",
      "          of the following places: within a NOTICE text file distributed\n",
      "          as part of the Derivative Works; within the Source form or\n",
      "          documentation, if provided along with the Derivative Works; or,\n",
      "          within a display generated by the Derivative Works, if and\n",
      "          wherever such third-party notices normally appear. The contents\n",
      "          of the NOTICE file are for informational purposes only and\n",
      "          do not modify the License. You may add Your own attribution\n",
      "          notices within Derivative Works that You distribute, alongside\n",
      "          or as an addendum to the NOTICE text from the Work, provided\n",
      "          that such additional attribution notices cannot be construed\n",
      "          as modifying the License.\n",
      "\n",
      "      You may add Your own copyright statement to Your modifications and\n",
      "      may provide additional or different license terms and conditions\n",
      "      for use, reproduction, or distribution of Your modifications, or\n",
      "      for any such Derivative Works as a whole, provided Your use,\n",
      "      reproduction, and distribution of the Work otherwise complies with\n",
      "      the conditions stated in this License.\n",
      "\n",
      "   5. Submission of Contributions. Unless You explicitly state otherwise,\n",
      "      any Contribution intentionally submitted for inclusion in the Work\n",
      "      by You to the Licensor shall be under the terms and conditions of\n",
      "      this License, without any additional terms or conditions.\n",
      "      Notwithstanding the above, nothing herein shall supersede or modify\n",
      "      the terms of any separate license agreement you may have executed\n",
      "      with Licensor regarding such Contributions.\n",
      "\n",
      "   6. Trademarks. This License does not grant permission to use the trade\n",
      "      names, trademarks, service marks, or product names of the Licensor,\n",
      "      except as required for reasonable and customary use in describing the\n",
      "      origin of the Work and reproducing the content of the NOTICE file.\n",
      "\n",
      "   7. Disclaimer of Warranty. Unless required by applicable law or\n",
      "      agreed to in writing, Licensor provides the Work (and each\n",
      "      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n",
      "      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
      "      implied, including, without limitation, any warranties or conditions\n",
      "      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n",
      "      PARTICULAR PURPOSE. You are solely responsible for determining the\n",
      "      appropriateness of using or redistributing the Work and assume any\n",
      "      risks associated with Your exercise of permissions under this License.\n",
      "\n",
      "   8. Limitation of Liability. In no event and under no legal theory,\n",
      "      whether in tort (including negligence), contract, or otherwise,\n",
      "      unless required by applicable law (such as deliberate and grossly\n",
      "      negligent acts) or agreed to in writing, shall any Contributor be\n",
      "      liable to You for damages, including any direct, indirect, special,\n",
      "      incidental, or consequential damages of any character arising as a\n",
      "      result of this License or out of the use or inability to use the\n",
      "      Work (including but not limited to damages for loss of goodwill,\n",
      "      work stoppage, computer failure or malfunction, or any and all\n",
      "      other commercial damages or losses), even if such Contributor\n",
      "      has been advised of the possibility of such damages.\n",
      "\n",
      "   9. Accepting Warranty or Additional Liability. While redistributing\n",
      "      the Work or Derivative Works thereof, You may choose to offer,\n",
      "      and charge a fee for, acceptance of support, warranty, indemnity,\n",
      "      or other liability obligations and/or rights consistent with this\n",
      "      License. However, in accepting such obligations, You may act only\n",
      "      on Your own behalf and on Your sole responsibility, not on behalf\n",
      "      of any other Contributor, and only if You agree to indemnify,\n",
      "      defend, and hold each Contributor harmless for any liability\n",
      "      incurred by, or claims asserted against, such Contributor by reason\n",
      "      of your accepting any such warranty or additional liability.\n",
      "\n",
      "   END OF TERMS AND CONDITIONS\n",
      "\n",
      "\n",
      "Copyright (c) Donald Stufft and individual contributors.\n",
      "All rights reserved.\n",
      "\n",
      "Redistribution and use in source and binary forms, with or without\n",
      "modification, are permitted provided that the following conditions are met:\n",
      "\n",
      "    1. Redistributions of source code must retain the above copyright notice,\n",
      "       this list of conditions and the following disclaimer.\n",
      "\n",
      "    2. Redistributions in binary form must reproduce the above copyright\n",
      "       notice, this list of conditions and the following disclaimer in the\n",
      "       documentation and/or other materials provided with the distribution.\n",
      "\n",
      "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
      "ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
      "WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.A. HISTORY OF THE SOFTWARE\n",
      "==========================\n",
      "\n",
      "Python was created in the early 1990s by Guido van Rossum at Stichting\n",
      "Mathematisch Centrum (CWI, see https://www.cwi.nl) in the Netherlands\n",
      "as a successor of a language called ABC.  Guido remains Python's\n",
      "principal author, although it includes many contributions from others.\n",
      "\n",
      "In 1995, Guido continued his work on Python at the Corporation for\n",
      "National Research Initiatives (CNRI, see https://www.cnri.reston.va.us)\n",
      "in Reston, Virginia where he released several versions of the\n",
      "software.\n",
      "\n",
      "In May 2000, Guido and the Python core development team moved to\n",
      "BeOpen.com to form the BeOpen PythonLabs team.  In October of the same\n",
      "year, the PythonLabs team moved to Digital Creations, which became\n",
      "Zope Corporation.  In 2001, the Python Software Foundation (PSF, see\n",
      "https://www.python.org/psf/) was formed, a non-profit organization\n",
      "created specifically to own Python-related Intellectual Property.\n",
      "Zope Corporation was a sponsoring member of the PSF.\n",
      "\n",
      "All Python releases are Open Source (see https://opensource.org for\n",
      "the Open Source Definition).  Historically, most, but not all, Python\n",
      "releases have also been GPL-compatible; the table below summarizes\n",
      "the various releases.\n",
      "\n",
      "    Release         Derived     Year        Owner       GPL-\n",
      "                    from                                compatible? (1)\n",
      "\n",
      "    0.9.0 thru 1.2              1991-1995   CWI         yes\n",
      "    1.3 thru 1.5.2  1.2         1995-1999   CNRI        yes\n",
      "    1.6             1.5.2       2000        CNRI        no\n",
      "    2.0             1.6         2000        BeOpen.com  no\n",
      "    1.6.1           1.6         2001        CNRI        yes (2)\n",
      "    2.1             2.0+1.6.1   2001        PSF         no\n",
      "    2.0.1           2.0+1.6.1   2001        PSF         yes\n",
      "    2.1.1           2.1+2.0.1   2001        PSF         yes\n",
      "    2.1.2           2.1.1       2002        PSF         yes\n",
      "    2.1.3           2.1.2       2002        PSF         yes\n",
      "    2.2 and above   2.1.1       2001-now    PSF         yes\n",
      "\n",
      "Footnotes:\n",
      "\n",
      "(1) GPL-compatible doesn't mean that we're distributing Python under\n",
      "    the GPL.  All Python licenses, unlike the GPL, let you distribute\n",
      "    a modified version without making your changes open source.  The\n",
      "    GPL-compatible licenses make it possible to combine Python with\n",
      "    other software that is released under the GPL; the others don't.\n",
      "\n",
      "(2) According to Richard Stallman, 1.6.1 is not GPL-compatible,\n",
      "    because its license has a choice of law clause.  According to\n",
      "    CNRI, however, Stallman's lawyer has told CNRI's lawyer that 1.6.1\n",
      "    is \"not incompatible\" with the GPL.\n",
      "\n",
      "Thanks to the many outside volunteers who have worked under Guido's\n",
      "direction to make these releases possible.\n",
      "\n",
      "\n",
      "B. TERMS AND CONDITIONS FOR ACCESSING OR OTHERWISE USING PYTHON\n",
      "===============================================================\n",
      "\n",
      "Python software and documentation are licensed under the\n",
      "Python Software Foundation License Version 2.\n",
      "\n",
      "Starting with Python 3.8.6, examples, recipes, and other code in\n",
      "the documentation are dual licensed under the PSF License Version 2\n",
      "and the Zero-Clause BSD license.\n",
      "\n",
      "Some software incorporated into Python is under different licenses.\n",
      "The licenses are listed with code falling under that license.\n",
      "\n",
      "\n",
      "PYTHON SOFTWARE FOUNDATION LICENSE VERSION 2\n",
      "--------------------------------------------\n",
      "\n",
      "1. This LICENSE AGREEMENT is between the Python Software Foundation\n",
      "(\"PSF\"), and the Individual or Organization (\"Licensee\") accessing and\n",
      "otherwise using this software (\"Python\") in source or binary form and\n",
      "its associated documentation.\n",
      "\n",
      "2. Subject to the terms and conditions of this License Agreement, PSF hereby\n",
      "grants Licensee a nonexclusive, royalty-free, world-wide license to reproduce,\n",
      "analyze, test, perform and/or display publicly, prepare derivative works,\n",
      "distribute, and otherwise use Python alone or in any derivative version,\n",
      "provided, however, that PSF's License Agreement and PSF's notice of copyright,\n",
      "i.e., \"Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010,\n",
      "2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023 Python Software Foundation;\n",
      "All Rights Reserved\" are retained in Python alone or in any derivative version\n",
      "prepared by Licensee.\n",
      "\n",
      "3. In the event Licensee prepares a derivative work that is based on\n",
      "or incorporates Python or any part thereof, and wants to make\n",
      "the derivative work available to others as provided herein, then\n",
      "Licensee hereby agrees to include in any such work a brief summary of\n",
      "the changes made to Python.\n",
      "\n",
      "4. PSF is making Python available to Licensee on an \"AS IS\"\n",
      "basis.  PSF MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR\n",
      "IMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, PSF MAKES NO AND\n",
      "DISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS\n",
      "FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF PYTHON WILL NOT\n",
      "INFRINGE ANY THIRD PARTY RIGHTS.\n",
      "\n",
      "5. PSF SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF PYTHON\n",
      "FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS\n",
      "A RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING PYTHON,\n",
      "OR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.\n",
      "\n",
      "6. This License Agreement will automatically terminate upon a material\n",
      "breach of its terms and conditions.\n",
      "\n",
      "7. Nothing in this License Agreement shall be deemed to create any\n",
      "relationship of agency, partnership, or joint venture between PSF and\n",
      "Licensee.  This License Agreement does not grant permission to use PSF\n",
      "trademarks or trade name in a trademark sense to endorse or promote\n",
      "products or services of Licensee, or any third party.\n",
      "\n",
      "8. By copying, installing or otherwise using Python, Licensee\n",
      "agrees to be bound by the terms and conditions of this License\n",
      "Agreement.\n",
      "\n",
      "\n",
      "BEOPEN.COM LICENSE AGREEMENT FOR PYTHON 2.0\n",
      "-------------------------------------------\n",
      "\n",
      "BEOPEN PYTHON OPEN SOURCE LICENSE AGREEMENT VERSION 1\n",
      "\n",
      "1. This LICENSE AGREEMENT is between BeOpen.com (\"BeOpen\"), having an\n",
      "office at 160 Saratoga Avenue, Santa Clara, CA 95051, and the\n",
      "Individual or Organization (\"Licensee\") accessing and otherwise using\n",
      "this software in source or binary form and its associated\n",
      "documentation (\"the Software\").\n",
      "\n",
      "2. Subject to the terms and conditions of this BeOpen Python License\n",
      "Agreement, BeOpen hereby grants Licensee a non-exclusive,\n",
      "royalty-free, world-wide license to reproduce, analyze, test, perform\n",
      "and/or display publicly, prepare derivative works, distribute, and\n",
      "otherwise use the Software alone or in any derivative version,\n",
      "provided, however, that the BeOpen Python License is retained in the\n",
      "Software, alone or in any derivative version prepared by Licensee.\n",
      "\n",
      "3. BeOpen is making the Software available to Licensee on an \"AS IS\"\n",
      "basis.  BEOPEN MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR\n",
      "IMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, BEOPEN MAKES NO AND\n",
      "DISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS\n",
      "FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF THE SOFTWARE WILL NOT\n",
      "INFRINGE ANY THIRD PARTY RIGHTS.\n",
      "\n",
      "4. BEOPEN SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF THE\n",
      "SOFTWARE FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS\n",
      "AS A RESULT OF USING, MODIFYING OR DISTRIBUTING THE SOFTWARE, OR ANY\n",
      "DERIVATIVE THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.\n",
      "\n",
      "5. This License Agreement will automatically terminate upon a material\n",
      "breach of its terms and conditions.\n",
      "\n",
      "6. This License Agreement shall be governed by and interpreted in all\n",
      "respects by the law of the State of California, excluding conflict of\n",
      "law provisions.  Nothing in this License Agreement shall be deemed to\n",
      "create any relationship of agency, partnership, or joint venture\n",
      "between BeOpen and Licensee.  This License Agreement does not grant\n",
      "permission to use BeOpen trademarks or trade names in a trademark\n",
      "sense to endorse or promote products or services of Licensee, or any\n",
      "third party.  As an exception, the \"BeOpen Python\" logos available at\n",
      "http://www.pythonlabs.com/logos.html may be used according to the\n",
      "permissions granted on that web page.\n",
      "\n",
      "7. By copying, installing or otherwise using the software, Licensee\n",
      "agrees to be bound by the terms and conditions of this License\n",
      "Agreement.\n",
      "\n",
      "\n",
      "CNRI LICENSE AGREEMENT FOR PYTHON 1.6.1\n",
      "---------------------------------------\n",
      "\n",
      "1. This LICENSE AGREEMENT is between the Corporation for National\n",
      "Research Initiatives, having an office at 1895 Preston White Drive,\n",
      "Reston, VA 20191 (\"CNRI\"), and the Individual or Organization\n",
      "(\"Licensee\") accessing and otherwise using Python 1.6.1 software in\n",
      "source or binary form and its associated documentation.\n",
      "\n",
      "2. Subject to the terms and conditions of this License Agreement, CNRI\n",
      "hereby grants Licensee a nonexclusive, royalty-free, world-wide\n",
      "license to reproduce, analyze, test, perform and/or display publicly,\n",
      "prepare derivative works, distribute, and otherwise use Python 1.6.1\n",
      "alone or in any derivative version, provided, however, that CNRI's\n",
      "License Agreement and CNRI's notice of copyright, i.e., \"Copyright (c)\n",
      "1995-2001 Corporation for National Research Initiatives; All Rights\n",
      "Reserved\" are retained in Python 1.6.1 alone or in any derivative\n",
      "version prepared by Licensee.  Alternately, in lieu of CNRI's License\n",
      "Agreement, Licensee may substitute the following text (omitting the\n",
      "quotes): \"Python 1.6.1 is made available subject to the terms and\n",
      "conditions in CNRI's License Agreement.  This Agreement together with\n",
      "Python 1.6.1 may be located on the internet using the following\n",
      "unique, persistent identifier (known as a handle): 1895.22/1013.  This\n",
      "Agreement may also be obtained from a proxy server on the internet\n",
      "using the following URL: http://hdl.handle.net/1895.22/1013\".\n",
      "\n",
      "3. In the event Licensee prepares a derivative work that is based on\n",
      "or incorporates Python 1.6.1 or any part thereof, and wants to make\n",
      "the derivative work available to others as provided herein, then\n",
      "Licensee hereby agrees to include in any such work a brief summary of\n",
      "the changes made to Python 1.6.1.\n",
      "\n",
      "4. CNRI is making Python 1.6.1 available to Licensee on an \"AS IS\"\n",
      "basis.  CNRI MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR\n",
      "IMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, CNRI MAKES NO AND\n",
      "DISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS\n",
      "FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF PYTHON 1.6.1 WILL NOT\n",
      "INFRINGE ANY THIRD PARTY RIGHTS.\n",
      "\n",
      "5. CNRI SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF PYTHON\n",
      "1.6.1 FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS\n",
      "A RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING PYTHON 1.6.1,\n",
      "OR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.\n",
      "\n",
      "6. This License Agreement will automatically terminate upon a material\n",
      "breach of its terms and conditions.\n",
      "\n",
      "7. This License Agreement shall be governed by the federal\n",
      "intellectual property law of the United States, including without\n",
      "limitation the federal copyright law, and, to the extent such\n",
      "U.S. federal law does not apply, by the law of the Commonwealth of\n",
      "Virginia, excluding Virginia's conflict of law provisions.\n",
      "Notwithstanding the foregoing, with regard to derivative works based\n",
      "on Python 1.6.1 that incorporate non-separable material that was\n",
      "previously distributed under the GNU General Public License (GPL), the\n",
      "law of the Commonwealth of Virginia shall govern this License\n",
      "Agreement only as to issues arising under or with respect to\n",
      "Paragraphs 4, 5, and 7 of this License Agreement.  Nothing in this\n",
      "License Agreement shall be deemed to create any relationship of\n",
      "agency, partnership, or joint venture between CNRI and Licensee.  This\n",
      "License Agreement does not grant permission to use CNRI trademarks or\n",
      "trade name in a trademark sense to endorse or promote products or\n",
      "services of Licensee, or any third party.\n",
      "\n",
      "8. By clicking on the \"ACCEPT\" button where indicated, or by copying,\n",
      "installing or otherwise using Python 1.6.1, Licensee agrees to be\n",
      "bound by the terms and conditions of this License Agreement.\n",
      "\n",
      "        ACCEPT\n",
      "\n",
      "\n",
      "CWI LICENSE AGREEMENT FOR PYTHON 0.9.0 THROUGH 1.2\n",
      "--------------------------------------------------\n",
      "\n",
      "Copyright (c) 1991 - 1995, Stichting Mathematisch Centrum Amsterdam,\n",
      "The Netherlands.  All rights reserved.\n",
      "\n",
      "Permission to use, copy, modify, and distribute this software and its\n",
      "documentation for any purpose and without fee is hereby granted,\n",
      "provided that the above copyright notice appear in all copies and that\n",
      "both that copyright notice and this permission notice appear in\n",
      "supporting documentation, and that the name of Stichting Mathematisch\n",
      "Centrum or CWI not be used in advertising or publicity pertaining to\n",
      "distribution of the software without specific, written prior\n",
      "permission.\n",
      "\n",
      "STICHTING MATHEMATISCH CENTRUM DISCLAIMS ALL WARRANTIES WITH REGARD TO\n",
      "THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND\n",
      "FITNESS, IN NO EVENT SHALL STICHTING MATHEMATISCH CENTRUM BE LIABLE\n",
      "FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n",
      "WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n",
      "ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n",
      "OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n",
      "\n",
      "ZERO-CLAUSE BSD LICENSE FOR CODE IN THE PYTHON DOCUMENTATION\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Permission to use, copy, modify, and/or distribute this software for any\n",
      "purpose with or without fee is hereby granted.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH\n",
      "REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY\n",
      "AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT,\n",
      "INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM\n",
      "LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR\n",
      "OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR\n",
      "PERFORMANCE OF THIS SOFTWARE.\n",
      "Copyright (c) 2014, Al Sweigart\n",
      "All rights reserved.\n",
      "\n",
      "Redistribution and use in source and binary forms, with or without\n",
      "modification, are permitted provided that the following conditions are met:\n",
      "\n",
      "* Redistributions of source code must retain the above copyright notice, this\n",
      "  list of conditions and the following disclaimer.\n",
      "\n",
      "* Redistributions in binary form must reproduce the above copyright notice,\n",
      "  this list of conditions and the following disclaimer in the documentation\n",
      "  and/or other materials provided with the distribution.\n",
      "\n",
      "* Neither the name of the {organization} nor the names of its\n",
      "  contributors may be used to endorse or promote products derived from\n",
      "  this software without specific prior written permission.\n",
      "\n",
      "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.Copyright (c) 2017 Anthony Sottile\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "of this software and associated documentation files (the \"Software\"), to deal\n",
      "in the Software without restriction, including without limitation the rights\n",
      "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "copies of the Software, and to permit persons to whom the Software is\n",
      "furnished to do so, subject to the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be included in\n",
      "all copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
      "THE SOFTWARE.Copyright (c) 2015-2019 Jared Hobbs\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
      "this software and associated documentation files (the \"Software\"), to deal in\n",
      "the Software without restriction, including without limitation the rights to\n",
      "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n",
      "of the Software, and to permit persons to whom the Software is furnished to do\n",
      "so, subject to the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be included in all\n",
      "copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "SOFTWARE.Developed by ESN, an Electronic Arts Inc. studio.\n",
      "Copyright (c) 2014, Electronic Arts Inc.\n",
      "All rights reserved.\n",
      "\n",
      "Redistribution and use in source and binary forms, with or without\n",
      "modification, are permitted provided that the following conditions are met:\n",
      "* Redistributions of source code must retain the above copyright\n",
      "notice, this list of conditions and the following disclaimer.\n",
      "* Redistributions in binary form must reproduce the above copyright\n",
      "notice, this list of conditions and the following disclaimer in the\n",
      "documentation and/or other materials provided with the distribution.\n",
      "* Neither the name of ESN, Electronic Arts Inc. nor the\n",
      "names of its contributors may be used to endorse or promote products\n",
      "derived from this software without specific prior written permission.\n",
      "\n",
      "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
      "ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
      "WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      "DISCLAIMED. IN NO EVENT SHALL ELECTRONIC ARTS INC. BE LIABLE\n",
      "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n",
      "(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n",
      "LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n",
      "ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
      "(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n",
      "SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "\n",
      "----\n",
      "\n",
      "Portions of code from MODP_ASCII - Ascii transformations (upper/lower, etc)\n",
      "https://github.com/client9/stringencoders\n",
      "\n",
      "  Copyright 2005, 2006, 2007\n",
      "  Nick Galbreath -- nickg [at] modp [dot] com\n",
      "  All rights reserved.\n",
      "\n",
      "  Redistribution and use in source and binary forms, with or without\n",
      "  modification, are permitted provided that the following conditions are\n",
      "  met:\n",
      "\n",
      "    Redistributions of source code must retain the above copyright\n",
      "    notice, this list of conditions and the following disclaimer.\n",
      "\n",
      "    Redistributions in binary form must reproduce the above copyright\n",
      "    notice, this list of conditions and the following disclaimer in the\n",
      "    documentation and/or other materials provided with the distribution.\n",
      "\n",
      "    Neither the name of the modp.com nor the names of its\n",
      "    contributors may be used to endorse or promote products derived from\n",
      "    this software without specific prior written permission.\n",
      "\n",
      "  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n",
      "  \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n",
      "  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n",
      "  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n",
      "  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n",
      "  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n",
      "  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n",
      "  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n",
      "  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
      "  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "\n",
      "  This is the standard \"new\" BSD license:\n",
      "  http://www.opensource.org/licenses/bsd-license.php\n",
      "\n",
      "https://github.com/client9/stringencoders/blob/cfd5c1507325ae497ea9bacdacba12c0ffd79d30/COPYING\n",
      "\n",
      "----\n",
      "\n",
      "Numeric decoder derived from from TCL library\n",
      "https://opensource.apple.com/source/tcl/tcl-14/tcl/license.terms\n",
      " * Copyright (c) 1988-1993 The Regents of the University of California.\n",
      " * Copyright (c) 1994 Sun Microsystems, Inc.\n",
      "\n",
      "  This software is copyrighted by the Regents of the University of\n",
      "  California, Sun Microsystems, Inc., Scriptics Corporation, ActiveState\n",
      "  Corporation and other parties.  The following terms apply to all files\n",
      "  associated with the software unless explicitly disclaimed in\n",
      "  individual files.\n",
      "\n",
      "  The authors hereby grant permission to use, copy, modify, distribute,\n",
      "  and license this software and its documentation for any purpose, provided\n",
      "  that existing copyright notices are retained in all copies and that this\n",
      "  notice is included verbatim in any distributions. No written agreement,\n",
      "  license, or royalty fee is required for any of the authorized uses.\n",
      "  Modifications to this software may be copyrighted by their authors\n",
      "  and need not follow the licensing terms described here, provided that\n",
      "  the new terms are clearly indicated on the first page of each file where\n",
      "  they apply.\n",
      "\n",
      "  IN NO EVENT SHALL THE AUTHORS OR DISTRIBUTORS BE LIABLE TO ANY PARTY\n",
      "  FOR DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES\n",
      "  ARISING OUT OF THE USE OF THIS SOFTWARE, ITS DOCUMENTATION, OR ANY\n",
      "  DERIVATIVES THEREOF, EVEN IF THE AUTHORS HAVE BEEN ADVISED OF THE\n",
      "  POSSIBILITY OF SUCH DAMAGE.\n",
      "\n",
      "  THE AUTHORS AND DISTRIBUTORS SPECIFICALLY DISCLAIM ANY WARRANTIES,\n",
      "  INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY,\n",
      "  FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT.  THIS SOFTWARE\n",
      "  IS PROVIDED ON AN \"AS IS\" BASIS, AND THE AUTHORS AND DISTRIBUTORS HAVE\n",
      "  NO OBLIGATION TO PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR\n",
      "  MODIFICATIONS.\n",
      "\n",
      "  GOVERNMENT USE: If you are acquiring this software on behalf of the\n",
      "  U.S. government, the Government shall have only \"Restricted Rights\"\n",
      "  in the software and related documentation as defined in the Federal\n",
      "  Acquisition Regulations (FARs) in Clause 52.227.19 (c) (2).  If you\n",
      "  are acquiring the software on behalf of the Department of Defense, the\n",
      "  software shall be classified as \"Commercial Computer Software\" and the\n",
      "  Government shall have only \"Restricted Rights\" as defined in Clause\n",
      "  252.227-7013 (c) (1) of DFARs.  Notwithstanding the foregoing, the\n",
      "  authors grant the U.S. Government and others acting in its behalf\n",
      "  permission to use and distribute the software in accordance with the\n",
      "  terms specified in this license.Apache License\n",
      "Version 2.0, January 2004\n",
      "http://www.apache.org/licenses/\n",
      "\n",
      "TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
      "\n",
      "1. Definitions.\n",
      "\n",
      "\"License\" shall mean the terms and conditions for use, reproduction, and\n",
      "distribution as defined by Sections 1 through 9 of this document.\n",
      "\n",
      "\"Licensor\" shall mean the copyright owner or entity authorized by the copyright\n",
      "owner that is granting the License.\n",
      "\n",
      "\"Legal Entity\" shall mean the union of the acting entity and all other entities\n",
      "that control, are controlled by, or are under common control with that entity.\n",
      "For the purposes of this definition, \"control\" means (i) the power, direct or\n",
      "indirect, to cause the direction or management of such entity, whether by\n",
      "contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the\n",
      "outstanding shares, or (iii) beneficial ownership of such entity.\n",
      "\n",
      "\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising\n",
      "permissions granted by this License.\n",
      "\n",
      "\"Source\" form shall mean the preferred form for making modifications, including\n",
      "but not limited to software source code, documentation source, and configuration\n",
      "files.\n",
      "\n",
      "\"Object\" form shall mean any form resulting from mechanical transformation or\n",
      "translation of a Source form, including but not limited to compiled object code,\n",
      "generated documentation, and conversions to other media types.\n",
      "\n",
      "\"Work\" shall mean the work of authorship, whether in Source or Object form, made\n",
      "available under the License, as indicated by a copyright notice that is included\n",
      "in or attached to the work (an example is provided in the Appendix below).\n",
      "\n",
      "\"Derivative Works\" shall mean any work, whether in Source or Object form, that\n",
      "is based on (or derived from) the Work and for which the editorial revisions,\n",
      "annotations, elaborations, or other modifications represent, as a whole, an\n",
      "original work of authorship. For the purposes of this License, Derivative Works\n",
      "shall not include works that remain separable from, or merely link (or bind by\n",
      "name) to the interfaces of, the Work and Derivative Works thereof.\n",
      "\n",
      "\"Contribution\" shall mean any work of authorship, including the original version\n",
      "of the Work and any modifications or additions to that Work or Derivative Works\n",
      "thereof, that is intentionally submitted to Licensor for inclusion in the Work\n",
      "by the copyright owner or by an individual or Legal Entity authorized to submit\n",
      "on behalf of the copyright owner. For the purposes of this definition,\n",
      "\"submitted\" means any form of electronic, verbal, or written communication sent\n",
      "to the Licensor or its representatives, including but not limited to\n",
      "communication on electronic mailing lists, source code control systems, and\n",
      "issue tracking systems that are managed by, or on behalf of, the Licensor for\n",
      "the purpose of discussing and improving the Work, but excluding communication\n",
      "that is conspicuously marked or otherwise designated in writing by the copyright\n",
      "owner as \"Not a Contribution.\"\n",
      "\n",
      "\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf\n",
      "of whom a Contribution has been received by Licensor and subsequently\n",
      "incorporated within the Work.\n",
      "\n",
      "2. Grant of Copyright License.\n",
      "\n",
      "Subject to the terms and conditions of this License, each Contributor hereby\n",
      "grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free,\n",
      "irrevocable copyright license to reproduce, prepare Derivative Works of,\n",
      "publicly display, publicly perform, sublicense, and distribute the Work and such\n",
      "Derivative Works in Source or Object form.\n",
      "\n",
      "3. Grant of Patent License.\n",
      "\n",
      "Subject to the terms and conditions of this License, each Contributor hereby\n",
      "grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free,\n",
      "irrevocable (except as stated in this section) patent license to make, have\n",
      "made, use, offer to sell, sell, import, and otherwise transfer the Work, where\n",
      "such license applies only to those patent claims licensable by such Contributor\n",
      "that are necessarily infringed by their Contribution(s) alone or by combination\n",
      "of their Contribution(s) with the Work to which such Contribution(s) was\n",
      "submitted. If You institute patent litigation against any entity (including a\n",
      "cross-claim or counterclaim in a lawsuit) alleging that the Work or a\n",
      "Contribution incorporated within the Work constitutes direct or contributory\n",
      "patent infringement, then any patent licenses granted to You under this License\n",
      "for that Work shall terminate as of the date such litigation is filed.\n",
      "\n",
      "4. Redistribution.\n",
      "\n",
      "You may reproduce and distribute copies of the Work or Derivative Works thereof\n",
      "in any medium, with or without modifications, and in Source or Object form,\n",
      "provided that You meet the following conditions:\n",
      "\n",
      "You must give any other recipients of the Work or Derivative Works a copy of\n",
      "this License; and\n",
      "You must cause any modified files to carry prominent notices stating that You\n",
      "changed the files; and\n",
      "You must retain, in the Source form of any Derivative Works that You distribute,\n",
      "all copyright, patent, trademark, and attribution notices from the Source form\n",
      "of the Work, excluding those notices that do not pertain to any part of the\n",
      "Derivative Works; and\n",
      "If the Work includes a \"NOTICE\" text file as part of its distribution, then any\n",
      "Derivative Works that You distribute must include a readable copy of the\n",
      "attribution notices contained within such NOTICE file, excluding those notices\n",
      "that do not pertain to any part of the Derivative Works, in at least one of the\n",
      "following places: within a NOTICE text file distributed as part of the\n",
      "Derivative Works; within the Source form or documentation, if provided along\n",
      "with the Derivative Works; or, within a display generated by the Derivative\n",
      "Works, if and wherever such third-party notices normally appear. The contents of\n",
      "the NOTICE file are for informational purposes only and do not modify the\n",
      "License. You may add Your own attribution notices within Derivative Works that\n",
      "You distribute, alongside or as an addendum to the NOTICE text from the Work,\n",
      "provided that such additional attribution notices cannot be construed as\n",
      "modifying the License.\n",
      "You may add Your own copyright statement to Your modifications and may provide\n",
      "additional or different license terms and conditions for use, reproduction, or\n",
      "distribution of Your modifications, or for any such Derivative Works as a whole,\n",
      "provided Your use, reproduction, and distribution of the Work otherwise complies\n",
      "with the conditions stated in this License.\n",
      "\n",
      "5. Submission of Contributions.\n",
      "\n",
      "Unless You explicitly state otherwise, any Contribution intentionally submitted\n",
      "for inclusion in the Work by You to the Licensor shall be under the terms and\n",
      "conditions of this License, without any additional terms or conditions.\n",
      "Notwithstanding the above, nothing herein shall supersede or modify the terms of\n",
      "any separate license agreement you may have executed with Licensor regarding\n",
      "such Contributions.\n",
      "\n",
      "6. Trademarks.\n",
      "\n",
      "This License does not grant permission to use the trade names, trademarks,\n",
      "service marks, or product names of the Licensor, except as required for\n",
      "reasonable and customary use in describing the origin of the Work and\n",
      "reproducing the content of the NOTICE file.\n",
      "\n",
      "7. Disclaimer of Warranty.\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, Licensor provides the\n",
      "Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied,\n",
      "including, without limitation, any warranties or conditions of TITLE,\n",
      "NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are\n",
      "solely responsible for determining the appropriateness of using or\n",
      "redistributing the Work and assume any risks associated with Your exercise of\n",
      "permissions under this License.\n",
      "\n",
      "8. Limitation of Liability.\n",
      "\n",
      "In no event and under no legal theory, whether in tort (including negligence),\n",
      "contract, or otherwise, unless required by applicable law (such as deliberate\n",
      "and grossly negligent acts) or agreed to in writing, shall any Contributor be\n",
      "liable to You for damages, including any direct, indirect, special, incidental,\n",
      "or consequential damages of any character arising as a result of this License or\n",
      "out of the use or inability to use the Work (including but not limited to\n",
      "damages for loss of goodwill, work stoppage, computer failure or malfunction, or\n",
      "any and all other commercial damages or losses), even if such Contributor has\n",
      "been advised of the possibility of such damages.\n",
      "\n",
      "9. Accepting Warranty or Additional Liability.\n",
      "\n",
      "While redistributing the Work or Derivative Works thereof, You may choose to\n",
      "offer, and charge a fee for, acceptance of support, warranty, indemnity, or\n",
      "other liability obligations and/or rights consistent with this License. However,\n",
      "in accepting such obligations, You may act only on Your own behalf and on Your\n",
      "sole responsibility, not on behalf of any other Contributor, and only if You\n",
      "agree to indemnify, defend, and hold each Contributor harmless for any liability\n",
      "incurred by, or claims asserted against, such Contributor by reason of your\n",
      "accepting any such warranty or additional liability.\n",
      "\n",
      "END OF TERMS AND CONDITIONS\n",
      "\n",
      "APPENDIX: How to apply the Apache License to your work\n",
      "\n",
      "To apply the Apache License to your work, attach the following boilerplate\n",
      "notice, with the fields enclosed by brackets \"[]\" replaced with your own\n",
      "identifying information. (Don't include the brackets!) The text should be\n",
      "enclosed in the appropriate comment syntax for the file format. We also\n",
      "recommend that a file or class name and description of purpose be included on\n",
      "the same \"printed page\" as the copyright notice for easier identification within\n",
      "third-party archives.\n",
      "\n",
      "   Copyright [yyyy] [name of copyright owner]\n",
      "\n",
      "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "   you may not use this file except in compliance with the License.\n",
      "   You may obtain a copy of the License at\n",
      "\n",
      "     http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "   Unless required by applicable law or agreed to in writing, software\n",
      "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "   See the License for the specific language governing permissions and\n",
      "   limitations under the License.\n",
      "Location: /home/codespace/.local/lib/python3.12/site-packages\n",
      "Requires: numpy, python-dateutil, pytz, tzdata\n",
      "Required-by: seaborn\n"
     ]
    }
   ],
   "source": [
    "!pip show pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96c16bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb980a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f96b86",
   "metadata": {},
   "source": [
    "# Guide to Using `pandas.read_csv`\n",
    "\n",
    "The `pandas.read_csv` function is one of the most commonly used tools in the Python **pandas** library for reading data from CSV (Comma-Separated Values) files into a **DataFrame**, a powerful data structure for data analysis. This guide will walk beginners through the basics of using `read_csv`, explaining its key parameters and providing practical examples to help you get started with loading and exploring data.\n",
    "\n",
    "## What is `pandas.read_csv`?\n",
    "\n",
    "The `read_csv` function reads a CSV file (or other delimited text files) and converts it into a **DataFrame**, which is like a table with rows and columns. CSV files are widely used for storing tabular data, such as spreadsheets or database exports, and `read_csv` makes it easy to import this data into Python for analysis.\n",
    "\n",
    "### Basic Syntax\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(filepath_or_buffer)\n",
    "```\n",
    "\n",
    "- **`filepath_or_buffer`**: The path to the CSV file (e.g., `\"data.csv\"`) or a file-like object (e.g., a URL or a file opened in Python).\n",
    "- **Returns**: A `DataFrame` (or a `TextFileReader` if using chunking or iteration).\n",
    "\n",
    "## Installing Pandas\n",
    "Before using `read_csv`, ensure you have pandas installed. You can install it using pip:\n",
    "```bash\n",
    "pip install pandas\n",
    "```\n",
    "\n",
    "## Key Parameters for Beginners\n",
    "The `read_csv` function has many parameters, but beginners should focus on the most commonly used ones to get started. Below, we explain these parameters and provide examples.\n",
    "\n",
    "### 1. **filepath_or_buffer**: Specifying the File\n",
    "This is the only required parameter, telling pandas where to find the CSV file. It can be:\n",
    "- A file path on your computer (e.g., `\"data.csv\"` or `\"C:/Users/YourName/data.csv\"`).\n",
    "- A URL pointing to a CSV file (e.g., `\"https://example.com/data.csv\"`).\n",
    "- A file-like object, such as a file opened with Python‚Äôs `open()` function.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Reading a local CSV file\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "print(df.head())  # Display the first 5 rows\n",
    "```\n",
    "\n",
    "If the file is hosted online:\n",
    "```python\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/datasets/sample-datasets/main/data.csv\")\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "### 2. **sep**: Specifying the Delimiter\n",
    "The `sep` parameter defines the character that separates values in the file. By default, it‚Äôs a comma (`,`), but you can specify other delimiters like tabs (`\\t`), semicolons (`;`), or spaces.\n",
    "\n",
    "**Example**:\n",
    "For a tab-separated file:\n",
    "```python\n",
    "df = pd.read_csv(\"data.txt\", sep=\"\\t\")\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "If the file uses semicolons:\n",
    "```python\n",
    "df = pd.read_csv(\"data.csv\", sep=\";\")\n",
    "```\n",
    "\n",
    "**Tip**: If you‚Äôre unsure about the delimiter, you can open the file in a text editor to check or use `sep=None` to let pandas guess the delimiter (though this uses the slower Python engine).\n",
    "\n",
    "### 3. **header**: Defining the Column Names\n",
    "The `header` parameter specifies which row contains the column names:\n",
    "- Default is `\"infer\"`, meaning pandas uses the first row as column names.\n",
    "- Set `header=0` to explicitly use the first row.\n",
    "- Set `header=None` if the file has no header row, and pandas will assign default column names (0, 1, 2, ...).\n",
    "\n",
    "**Example**:\n",
    "For a CSV with no header:\n",
    "```python\n",
    "df = pd.read_csv(\"data.csv\", header=None)\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "You can also provide custom column names using the `names` parameter (see below).\n",
    "\n",
    "### 4. **names**: Custom Column Names\n",
    "If your CSV file doesn‚Äôt have a header or you want to override the existing header, use the `names` parameter to provide a list of column names. Combine with `header=0` to skip the file‚Äôs header row.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "df = pd.read_csv(\"data.csv\", header=0, names=[\"ID\", \"Name\", \"Age\"])\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "### 5. **index_col**: Setting the Index\n",
    "The `index_col` parameter specifies which column(s) to use as the DataFrame‚Äôs index (row labels). You can pass a column name, index (e.g., 0 for the first column), or `False` to prevent any column from being used as the index.\n",
    "\n",
    "**Example**:\n",
    "Use the \"ID\" column as the index:\n",
    "```python\n",
    "df = pd.read_csv(\"data.csv\", index_col=\"ID\")\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "### 6. **usecols**: Selecting Specific Columns\n",
    "The `usecols` parameter lets you load only specific columns, which is useful for large files to save memory. Provide a list of column names or indices.\n",
    "\n",
    "**Example**:\n",
    "Load only the \"Name\" and \"Age\" columns:\n",
    "```python\n",
    "df = pd.read_csv(\"data.csv\", usecols=[\"Name\", \"Age\"])\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "### 7. **dtype**: Specifying Data Types\n",
    "The `dtype` parameter lets you define the data type for columns (e.g., `int`, `float`, `str`). By default, pandas infers types, but you can override this to ensure correctness or save memory.\n",
    "\n",
    "**Example**:\n",
    "Force the \"Age\" column to be an integer:\n",
    "```python\n",
    "df = pd.read_csv(\"data.csv\", dtype={\"Age\": int})\n",
    "print(df.dtypes)\n",
    "```\n",
    "\n",
    "### 8. **na_values**: Handling Missing Values\n",
    "The `na_values` parameter specifies values to treat as `NaN` (missing). By default, pandas recognizes values like `\"NA\"`, `\"NaN\"`, `\"null\"`, etc. You can add custom values.\n",
    "\n",
    "**Example**:\n",
    "Treat \"missing\" and \"-\" as `NaN`:\n",
    "```python\n",
    "df = pd.read_csv(\"data.csv\", na_values=[\"missing\", \"-\"])\n",
    "print(df.isna().sum())\n",
    "```\n",
    "\n",
    "### 9. **skiprows**: Skipping Rows\n",
    "Use `skiprows` to skip specific rows at the start of the file (e.g., metadata or comments). Pass an integer (number of rows to skip) or a list of row indices.\n",
    "\n",
    "**Example**:\n",
    "Skip the first 2 rows:\n",
    "```python\n",
    "df = pd.read_csv(\"data.csv\", skiprows=2)\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "### 10. **nrows**: Limiting Rows\n",
    "The `nrows` parameter limits the number of rows to read, which is helpful for testing with large files.\n",
    "\n",
    "**Example**:\n",
    "Read only the first 100 rows:\n",
    "```python\n",
    "df = pd.read_csv(\"data.csv\", nrows=100)\n",
    "print(df.shape)\n",
    "```\n",
    "\n",
    "### 11. **encoding**: Handling File Encoding\n",
    "Some CSV files use non-standard encodings (e.g., `\"latin1\"` instead of `\"utf-8\"`). Use the `encoding` parameter to specify the correct encoding.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "df = pd.read_csv(\"data.csv\", encoding=\"latin1\")\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "**Tip**: Common encodings include `\"utf-8\"`, `\"latin1\"`, and `\"iso-8859-1\"`. Check your file‚Äôs encoding if you get errors.\n",
    "\n",
    "## Practical Example\n",
    "Suppose you have a CSV file named `students.csv` with the following content:\n",
    "```\n",
    "ID,Name,Age,Grade\n",
    "1,Alice,20,A\n",
    "2,Bob,21,B\n",
    "3,Charlie,19,A\n",
    "4,,20,C\n",
    "```\n",
    "\n",
    "Here‚Äôs how to load and clean it:\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV with custom settings\n",
    "df = pd.read_csv(\"students.csv\", \n",
    "                 index_col=\"ID\", \n",
    "                 na_values=[\"\"], \n",
    "                 dtype={\"Age\": int, \"Grade\": str})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Display data types\n",
    "print(df.dtypes)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```\n",
    "       Name  Age Grade\n",
    "ID                    \n",
    "1     Alice   20     A\n",
    "2       Bob   21     B\n",
    "3   Charlie   19     A\n",
    "4       NaN   20     C\n",
    "\n",
    "Name     1\n",
    "Age      0\n",
    "Grade    0\n",
    "dtype: int64\n",
    "\n",
    "Name     object\n",
    "Age       int32\n",
    "Grade    object\n",
    "dtype: object\n",
    "```\n",
    "\n",
    "## Tips for Beginners\n",
    "1. **Check Your File First**: Open your CSV in a text editor to understand its structure (delimiter, header, encoding, etc.).\n",
    "2. **Start Simple**: Use default settings initially, then add parameters as needed.\n",
    "3. **Handle Missing Data**: Use `na_values` and check for `NaN` with `df.isna().sum()`.\n",
    "4. **Explore the DataFrame**: After loading, use `df.head()`, `df.info()`, and `df.describe()` to inspect your data.\n",
    "5. **Read the Docs**: For advanced use cases, refer to the [pandas IO Tools documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html).\n",
    "\n",
    "## Common Issues and Solutions\n",
    "- **FileNotFoundError**: Ensure the file path is correct and the file exists.\n",
    "- **UnicodeDecodeError**: Try a different `encoding` (e.g., `\"latin1\"` or `\"iso-8859-1\"`).\n",
    "- **ParserError**: Check if the delimiter (`sep`) matches the file‚Äôs format.\n",
    "- **Memory Issues**: Use `nrows`, `usecols`, or `chunksize` for large files.\n",
    "\n",
    "## Advanced Features (For Later)\n",
    "Once you‚Äôre comfortable with the basics, explore these parameters:\n",
    "- **parse_dates**: Parse columns as dates.\n",
    "- **chunksize**: Read large files in chunks for memory efficiency.\n",
    "- **converters**: Apply custom functions to columns during loading.\n",
    "- **compression**: Handle compressed files (e.g., `.gz`, `.zip`).\n",
    "\n",
    "## Conclusion\n",
    "The `pandas.read_csv` function is a versatile tool for loading CSV data into a DataFrame. By mastering the key parameters covered in this guide, you‚Äôll be well-equipped to handle most CSV files. Start with simple imports, experiment with parameters, and explore your data to unlock the power of pandas for data analysis!\n",
    "\n",
    "For more details, check the official [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfcf372",
   "metadata": {},
   "source": [
    "# Unions -> DataFrame | TextFileReader\n",
    "```python\n",
    "@overload\n",
    "def read_csv(\n",
    "    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n",
    "    *,\n",
    "    sep: str | None | lib.NoDefault = ...,\n",
    "    delimiter: str | None | lib.NoDefault = ...,\n",
    "    header: int | Sequence[int] | None | Literal[\"infer\"] = ...,\n",
    "    names: Sequence[Hashable] | None | lib.NoDefault = ...,\n",
    "    index_col: IndexLabel | Literal[False] | None = ...,\n",
    "    usecols: UsecolsArgType = ...,\n",
    "    dtype: DtypeArg | None = ...,\n",
    "    engine: CSVEngine | None = ...,\n",
    "    converters: Mapping[Hashable, Callable] | None = ...,\n",
    "    true_values: list | None = ...,\n",
    "    false_values: list | None = ...,\n",
    "    skipinitialspace: bool = ...,\n",
    "    skiprows: list[int] | int | Callable[[Hashable], bool] | None = ...,\n",
    "    skipfooter: int = ...,\n",
    "    nrows: int | None = ...,\n",
    "    na_values: Hashable\n",
    "    | Iterable[Hashable]\n",
    "    | Mapping[Hashable, Iterable[Hashable]]\n",
    "    | None = ...,\n",
    "    keep_default_na: bool = ...,\n",
    "    na_filter: bool = ...,\n",
    "    verbose: bool | lib.NoDefault = ...,\n",
    "    skip_blank_lines: bool = ...,\n",
    "    parse_dates: bool | Sequence[Hashable] | None = ...,\n",
    "    infer_datetime_format: bool | lib.NoDefault = ...,\n",
    "    keep_date_col: bool | lib.NoDefault = ...,\n",
    "    date_parser: Callable | lib.NoDefault = ...,\n",
    "    date_format: str | dict[Hashable, str] | None = ...,\n",
    "    dayfirst: bool = ...,\n",
    "    cache_dates: bool = ...,\n",
    "    iterator: bool = ...,\n",
    "    chunksize: int | None = ...,\n",
    "    compression: CompressionOptions = ...,\n",
    "    thousands: str | None = ...,\n",
    "    decimal: str = ...,\n",
    "    lineterminator: str | None = ...,\n",
    "    quotechar: str = ...,\n",
    "    quoting: int = ...,\n",
    "    doublequote: bool = ...,\n",
    "    escapechar: str | None = ...,\n",
    "    comment: str | None = ...,\n",
    "    encoding: str | None = ...,\n",
    "    encoding_errors: str | None = ...,\n",
    "    dialect: str | csv.Dialect | None = ...,\n",
    "    on_bad_lines=...,\n",
    "    delim_whitespace: bool | lib.NoDefault = ...,\n",
    "    low_memory: bool = ...,\n",
    "    memory_map: bool = ...,\n",
    "    float_precision: Literal[\"high\", \"legacy\"] | None = ...,\n",
    "    storage_options: StorageOptions = ...,\n",
    "    dtype_backend: DtypeBackend | lib.NoDefault = ...,\n",
    ") -> DataFrame | TextFileReader:\n",
    "    ...\n",
    "\n",
    "\n",
    "@Appender(\n",
    "    _doc_read_csv_and_table.format(\n",
    "        func_name=\"read_csv\",\n",
    "        summary=\"Read a comma-separated values (csv) file into DataFrame.\",\n",
    "        see_also_func_name=\"read_table\",\n",
    "        see_also_func_summary=\"Read general delimited file into DataFrame.\",\n",
    "        _default_sep=\"','\",\n",
    "        storage_options=_shared_docs[\"storage_options\"],\n",
    "        decompression_options=_shared_docs[\"decompression_options\"]\n",
    "        % \"filepath_or_buffer\",\n",
    "    )\n",
    ")\n",
    "def read_csv(\n",
    "    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n",
    "    *,\n",
    "    sep: str | None | lib.NoDefault = lib.no_default,\n",
    "    delimiter: str | None | lib.NoDefault = None,\n",
    "    # Column and Index Locations and Names\n",
    "    header: int | Sequence[int] | None | Literal[\"infer\"] = \"infer\",\n",
    "    names: Sequence[Hashable] | None | lib.NoDefault = lib.no_default,\n",
    "    index_col: IndexLabel | Literal[False] | None = None,\n",
    "    usecols: UsecolsArgType = None,\n",
    "    # General Parsing Configuration\n",
    "    dtype: DtypeArg | None = None,\n",
    "    engine: CSVEngine | None = None,\n",
    "    converters: Mapping[Hashable, Callable] | None = None,\n",
    "    true_values: list | None = None,\n",
    "    false_values: list | None = None,\n",
    "    skipinitialspace: bool = False,\n",
    "    skiprows: list[int] | int | Callable[[Hashable], bool] | None = None,\n",
    "    skipfooter: int = 0,\n",
    "    nrows: int | None = None,\n",
    "    # NA and Missing Data Handling\n",
    "    na_values: Hashable\n",
    "    | Iterable[Hashable]\n",
    "    | Mapping[Hashable, Iterable[Hashable]]\n",
    "    | None = None,\n",
    "    keep_default_na: bool = True,\n",
    "    na_filter: bool = True,\n",
    "    verbose: bool | lib.NoDefault = lib.no_default,\n",
    "    skip_blank_lines: bool = True,\n",
    "    # Datetime Handling\n",
    "    parse_dates: bool | Sequence[Hashable] | None = None,\n",
    "    infer_datetime_format: bool | lib.NoDefault = lib.no_default,\n",
    "    keep_date_col: bool | lib.NoDefault = lib.no_default,\n",
    "    date_parser: Callable | lib.NoDefault = lib.no_default,\n",
    "    date_format: str | dict[Hashable, str] | None = None,\n",
    "    dayfirst: bool = False,\n",
    "    cache_dates: bool = True,\n",
    "    # Iteration\n",
    "    iterator: bool = False,\n",
    "    chunksize: int | None = None,\n",
    "    # Quoting, Compression, and File Format\n",
    "    compression: CompressionOptions = \"infer\",\n",
    "    thousands: str | None = None,\n",
    "    decimal: str = \".\",\n",
    "    lineterminator: str | None = None,\n",
    "    quotechar: str = '\"',\n",
    "    quoting: int = csv.QUOTE_MINIMAL,\n",
    "    doublequote: bool = True,\n",
    "    escapechar: str | None = None,\n",
    "    comment: str | None = None,\n",
    "    encoding: str | None = None,\n",
    "    encoding_errors: str | None = \"strict\",\n",
    "    dialect: str | csv.Dialect | None = None,\n",
    "    # Error Handling\n",
    "    on_bad_lines: str = \"error\",\n",
    "    # Internal\n",
    "    delim_whitespace: bool | lib.NoDefault = lib.no_default,\n",
    "    low_memory: bool = _c_parser_defaults[\"low_memory\"],\n",
    "    memory_map: bool = False,\n",
    "    float_precision: Literal[\"high\", \"legacy\"] | None = None,\n",
    "    storage_options: StorageOptions | None = None,\n",
    "    dtype_backend: DtypeBackend | lib.NoDefault = lib.no_default,\n",
    ") -> DataFrame | TextFileReader:\n",
    "    if keep_date_col is not lib.no_default:\n",
    "        # GH#55569\n",
    "        warnings.warn(\n",
    "            \"The 'keep_date_col' keyword in pd.read_csv is deprecated and \"\n",
    "            \"will be removed in a future version. Explicitly remove unwanted \"\n",
    "            \"columns after parsing instead.\",\n",
    "            FutureWarning,\n",
    "            stacklevel=find_stack_level(),\n",
    "        )\n",
    "    else:\n",
    "        keep_date_col = False\n",
    "\n",
    "    if lib.is_list_like(parse_dates):\n",
    "        # GH#55569\n",
    "        depr = False\n",
    "        # error: Item \"bool\" of \"bool | Sequence[Hashable] | None\" has no\n",
    "        # attribute \"__iter__\" (not iterable)\n",
    "        if not all(is_hashable(x) for x in parse_dates):  # type: ignore[union-attr]\n",
    "            depr = True\n",
    "        elif isinstance(parse_dates, dict) and any(\n",
    "            lib.is_list_like(x) for x in parse_dates.values()\n",
    "        ):\n",
    "            depr = True\n",
    "        if depr:\n",
    "            warnings.warn(\n",
    "                \"Support for nested sequences for 'parse_dates' in pd.read_csv \"\n",
    "                \"is deprecated. Combine the desired columns with pd.to_datetime \"\n",
    "                \"after parsing instead.\",\n",
    "                FutureWarning,\n",
    "                stacklevel=find_stack_level(),\n",
    "            )\n",
    "\n",
    "    if infer_datetime_format is not lib.no_default:\n",
    "        warnings.warn(\n",
    "            \"The argument 'infer_datetime_format' is deprecated and will \"\n",
    "            \"be removed in a future version. \"\n",
    "            \"A strict version of it is now the default, see \"\n",
    "            \"https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. \"\n",
    "            \"You can safely remove this argument.\",\n",
    "            FutureWarning,\n",
    "            stacklevel=find_stack_level(),\n",
    "        )\n",
    "\n",
    "    if delim_whitespace is not lib.no_default:\n",
    "        # GH#55569\n",
    "        warnings.warn(\n",
    "            \"The 'delim_whitespace' keyword in pd.read_csv is deprecated and \"\n",
    "            \"will be removed in a future version. Use ``sep='\\\\s+'`` instead\",\n",
    "            FutureWarning,\n",
    "            stacklevel=find_stack_level(),\n",
    "        )\n",
    "    else:\n",
    "        delim_whitespace = False\n",
    "\n",
    "    if verbose is not lib.no_default:\n",
    "        # GH#55569\n",
    "        warnings.warn(\n",
    "            \"The 'verbose' keyword in pd.read_csv is deprecated and \"\n",
    "            \"will be removed in a future version.\",\n",
    "            FutureWarning,\n",
    "            stacklevel=find_stack_level(),\n",
    "        )\n",
    "    else:\n",
    "        verbose = False\n",
    "\n",
    "    # locals() should never be modified\n",
    "    kwds = locals().copy()\n",
    "    del kwds[\"filepath_or_buffer\"]\n",
    "    del kwds[\"sep\"]\n",
    "\n",
    "    kwds_defaults = _refine_defaults_read(\n",
    "        dialect,\n",
    "        delimiter,\n",
    "        delim_whitespace,\n",
    "        engine,\n",
    "        sep,\n",
    "        on_bad_lines,\n",
    "        names,\n",
    "        defaults={\"delimiter\": \",\"},\n",
    "        dtype_backend=dtype_backend,\n",
    "    )\n",
    "    kwds.update(kwds_defaults)\n",
    "\n",
    "    return _read(filepath_or_buffer, kwds)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba6a94",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d778926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "678ae0c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2e15d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "Module contains tools for processing files into DataFrames or other objects\n",
      "\n",
      "GH#48849 provides a convenient way of deprecating keyword arguments\n",
      "\"\"\"\n",
      "from __future__ import annotations\n",
      "\n",
      "from collections import (\n",
      "    abc,\n",
      "    defaultdict,\n",
      ")\n",
      "import csv\n",
      "import sys\n",
      "from textwrap import fill\n",
      "from typing import (\n",
      "    IO,\n",
      "    TYPE_CHECKING,\n",
      "    Any,\n",
      "    Callable,\n",
      "    Literal,\n",
      "    NamedTuple,\n",
      "    TypedDict,\n",
      "    overload,\n",
      ")\n",
      "import warnings\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "from pandas._config import using_copy_on_write\n",
      "\n",
      "from pandas._libs import lib\n",
      "from pandas._libs.parsers import STR_NA_VALUES\n",
      "from pandas.errors import (\n",
      "    AbstractMethodError,\n",
      "    ParserWarning,\n",
      ")\n",
      "from pandas.util._decorators import Appender\n",
      "from pandas.util._exceptions import find_stack_level\n",
      "from pandas.util._validators import check_dtype_backend\n",
      "\n",
      "from pandas.core.dtypes.common import (\n",
      "    is_file_like,\n",
      "    is_float,\n",
      "    is_hashable,\n",
      "    is_integer,\n",
      "    is_list_like,\n",
      "    pandas_dtype,\n",
      ")\n",
      "\n",
      "from pandas import Series\n",
      "from pandas.core.frame import DataFrame\n",
      "from pandas.core.indexes.api import RangeIndex\n",
      "from pandas.core.shared_docs import _shared_docs\n",
      "\n",
      "from pandas.io.common import (\n",
      "    IOHandles,\n",
      "    get_handle,\n",
      "    stringify_path,\n",
      "    validate_header_arg,\n",
      ")\n",
      "from pandas.io.parsers.arrow_parser_wrapper import ArrowParserWrapper\n",
      "from pandas.io.parsers.base_parser import (\n",
      "    ParserBase,\n",
      "    is_index_col,\n",
      "    parser_defaults,\n",
      ")\n",
      "from pandas.io.parsers.c_parser_wrapper import CParserWrapper\n",
      "from pandas.io.parsers.python_parser import (\n",
      "    FixedWidthFieldParser,\n",
      "    PythonParser,\n",
      ")\n",
      "\n",
      "if TYPE_CHECKING:\n",
      "    from collections.abc import (\n",
      "        Hashable,\n",
      "        Iterable,\n",
      "        Mapping,\n",
      "        Sequence,\n",
      "    )\n",
      "    from types import TracebackType\n",
      "\n",
      "    from pandas._typing import (\n",
      "        CompressionOptions,\n",
      "        CSVEngine,\n",
      "        DtypeArg,\n",
      "        DtypeBackend,\n",
      "        FilePath,\n",
      "        IndexLabel,\n",
      "        ReadCsvBuffer,\n",
      "        Self,\n",
      "        StorageOptions,\n",
      "        UsecolsArgType,\n",
      "    )\n",
      "_doc_read_csv_and_table = (\n",
      "    r\"\"\"\n",
      "{summary}\n",
      "\n",
      "Also supports optionally iterating or breaking of the file\n",
      "into chunks.\n",
      "\n",
      "Additional help can be found in the online docs for\n",
      "`IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "filepath_or_buffer : str, path object or file-like object\n",
      "    Any valid string path is acceptable. The string could be a URL. Valid\n",
      "    URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
      "    expected. A local file could be: file://localhost/path/to/table.csv.\n",
      "\n",
      "    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
      "\n",
      "    By file-like object, we refer to objects with a ``read()`` method, such as\n",
      "    a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
      "sep : str, default {_default_sep}\n",
      "    Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n",
      "    C engine cannot automatically detect\n",
      "    the separator, but the Python parsing engine can, meaning the latter will\n",
      "    be used and automatically detect the separator from only the first valid\n",
      "    row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n",
      "    In addition, separators longer than 1 character and different from\n",
      "    ``'\\s+'`` will be interpreted as regular expressions and will also force\n",
      "    the use of the Python parsing engine. Note that regex delimiters are prone\n",
      "    to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
      "delimiter : str, optional\n",
      "    Alias for ``sep``.\n",
      "header : int, Sequence of int, 'infer' or None, default 'infer'\n",
      "    Row number(s) containing column labels and marking the start of the\n",
      "    data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n",
      "    are passed the behavior is identical to ``header=0`` and column\n",
      "    names are inferred from the first line of the file, if column\n",
      "    names are passed explicitly to ``names`` then the behavior is identical to\n",
      "    ``header=None``. Explicitly pass ``header=0`` to be able to\n",
      "    replace existing names. The header can be a list of integers that\n",
      "    specify row locations for a :class:`~pandas.MultiIndex` on the columns\n",
      "    e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n",
      "    skipped (e.g. 2 in this example is skipped). Note that this\n",
      "    parameter ignores commented lines and empty lines if\n",
      "    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
      "    data rather than the first line of the file.\n",
      "names : Sequence of Hashable, optional\n",
      "    Sequence of column labels to apply. If the file contains a header row,\n",
      "    then you should explicitly pass ``header=0`` to override the column names.\n",
      "    Duplicates in this list are not allowed.\n",
      "index_col : Hashable, Sequence of Hashable or False, optional\n",
      "  Column(s) to use as row label(s), denoted either by column labels or column\n",
      "  indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n",
      "  will be formed for the row labels.\n",
      "\n",
      "  Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
      "  column as the index, e.g., when you have a malformed file with delimiters at\n",
      "  the end of each line.\n",
      "usecols : Sequence of Hashable or Callable, optional\n",
      "    Subset of columns to select, denoted either by column labels or column indices.\n",
      "    If list-like, all elements must either\n",
      "    be positional (i.e. integer indices into the document columns) or strings\n",
      "    that correspond to column names provided either by the user in ``names`` or\n",
      "    inferred from the document header row(s). If ``names`` are given, the document\n",
      "    header row(s) are not taken into account. For example, a valid list-like\n",
      "    ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
      "    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
      "    To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n",
      "    preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n",
      "    for columns in ``['foo', 'bar']`` order or\n",
      "    ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
      "    for ``['bar', 'foo']`` order.\n",
      "\n",
      "    If callable, the callable function will be evaluated against the column\n",
      "    names, returning names where the callable function evaluates to ``True``. An\n",
      "    example of a valid callable argument would be ``lambda x: x.upper() in\n",
      "    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
      "    parsing time and lower memory usage.\n",
      "dtype : dtype or dict of {{Hashable : dtype}}, optional\n",
      "    Data type(s) to apply to either the whole dataset or individual columns.\n",
      "    E.g., ``{{'a': np.float64, 'b': np.int32, 'c': 'Int64'}}``\n",
      "    Use ``str`` or ``object`` together with suitable ``na_values`` settings\n",
      "    to preserve and not interpret ``dtype``.\n",
      "    If ``converters`` are specified, they will be applied INSTEAD\n",
      "    of ``dtype`` conversion.\n",
      "\n",
      "    .. versionadded:: 1.5.0\n",
      "\n",
      "        Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n",
      "        the default determines the ``dtype`` of the columns which are not explicitly\n",
      "        listed.\n",
      "engine : {{'c', 'python', 'pyarrow'}}, optional\n",
      "    Parser engine to use. The C and pyarrow engines are faster, while the python engine\n",
      "    is currently more feature-complete. Multithreading is currently only supported by\n",
      "    the pyarrow engine.\n",
      "\n",
      "    .. versionadded:: 1.4.0\n",
      "\n",
      "        The 'pyarrow' engine was added as an *experimental* engine, and some features\n",
      "        are unsupported, or may not work correctly, with this engine.\n",
      "converters : dict of {{Hashable : Callable}}, optional\n",
      "    Functions for converting values in specified columns. Keys can either\n",
      "    be column labels or column indices.\n",
      "true_values : list, optional\n",
      "    Values to consider as ``True`` in addition to case-insensitive variants of 'True'.\n",
      "false_values : list, optional\n",
      "    Values to consider as ``False`` in addition to case-insensitive variants of 'False'.\n",
      "skipinitialspace : bool, default False\n",
      "    Skip spaces after delimiter.\n",
      "skiprows : int, list of int or Callable, optional\n",
      "    Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n",
      "    at the start of the file.\n",
      "\n",
      "    If callable, the callable function will be evaluated against the row\n",
      "    indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n",
      "    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
      "skipfooter : int, default 0\n",
      "    Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\n",
      "nrows : int, optional\n",
      "    Number of rows of file to read. Useful for reading pieces of large files.\n",
      "na_values : Hashable, Iterable of Hashable or dict of {{Hashable : Iterable}}, optional\n",
      "    Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n",
      "    per-column ``NA`` values.  By default the following values are interpreted as\n",
      "    ``NaN``: \" \"\"\"\n",
      "    + fill('\", \"'.join(sorted(STR_NA_VALUES)), 70, subsequent_indent=\"    \")\n",
      "    + \"\"\" \".\n",
      "\n",
      "keep_default_na : bool, default True\n",
      "    Whether or not to include the default ``NaN`` values when parsing the data.\n",
      "    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n",
      "\n",
      "    * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n",
      "      is appended to the default ``NaN`` values used for parsing.\n",
      "    * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n",
      "      the default ``NaN`` values are used for parsing.\n",
      "    * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n",
      "      the ``NaN`` values specified ``na_values`` are used for parsing.\n",
      "    * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n",
      "      strings will be parsed as ``NaN``.\n",
      "\n",
      "    Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n",
      "    ``na_values`` parameters will be ignored.\n",
      "na_filter : bool, default True\n",
      "    Detect missing value markers (empty strings and the value of ``na_values``). In\n",
      "    data without any ``NA`` values, passing ``na_filter=False`` can improve the\n",
      "    performance of reading a large file.\n",
      "verbose : bool, default False\n",
      "    Indicate number of ``NA`` values placed in non-numeric columns.\n",
      "\n",
      "    .. deprecated:: 2.2.0\n",
      "skip_blank_lines : bool, default True\n",
      "    If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\n",
      "parse_dates : bool, list of Hashable, list of lists or dict of {{Hashable : list}}, \\\n",
      "default False\n",
      "    The behavior is as follows:\n",
      "\n",
      "    * ``bool``. If ``True`` -> try parsing the index. Note: Automatically set to\n",
      "      ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n",
      "    * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n",
      "      each as a separate date column.\n",
      "    * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -> combine columns 1 and 3 and parse\n",
      "      as a single date column. Values are joined with a space before parsing.\n",
      "    * ``dict``, e.g. ``{{'foo' : [1, 3]}}`` -> parse columns 1, 3 as date and call\n",
      "      result 'foo'. Values are joined with a space before parsing.\n",
      "\n",
      "    If a column or index cannot be represented as an array of ``datetime``,\n",
      "    say because of an unparsable value or a mixture of timezones, the column\n",
      "    or index will be returned unaltered as an ``object`` data type. For\n",
      "    non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n",
      "    :func:`~pandas.read_csv`.\n",
      "\n",
      "    Note: A fast-path exists for iso8601-formatted dates.\n",
      "infer_datetime_format : bool, default False\n",
      "    If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n",
      "    format of the ``datetime`` strings in the columns, and if it can be inferred,\n",
      "    switch to a faster method of parsing them. In some cases this can increase\n",
      "    the parsing speed by 5-10x.\n",
      "\n",
      "    .. deprecated:: 2.0.0\n",
      "        A strict version of this argument is now the default, passing it has no effect.\n",
      "\n",
      "keep_date_col : bool, default False\n",
      "    If ``True`` and ``parse_dates`` specifies combining multiple columns then\n",
      "    keep the original columns.\n",
      "date_parser : Callable, optional\n",
      "    Function to use for converting a sequence of string columns to an array of\n",
      "    ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n",
      "    conversion. pandas will try to call ``date_parser`` in three different ways,\n",
      "    advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "    (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n",
      "    string values from the columns defined by ``parse_dates`` into a single array\n",
      "    and pass that; and 3) call ``date_parser`` once for each row using one or\n",
      "    more strings (corresponding to the columns defined by ``parse_dates``) as\n",
      "    arguments.\n",
      "\n",
      "    .. deprecated:: 2.0.0\n",
      "       Use ``date_format`` instead, or read in as ``object`` and then apply\n",
      "       :func:`~pandas.to_datetime` as-needed.\n",
      "date_format : str or dict of column -> format, optional\n",
      "    Format to use for parsing dates when used in conjunction with ``parse_dates``.\n",
      "    The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n",
      "    `strftime documentation\n",
      "    <https://docs.python.org/3/library/datetime.html\n",
      "    #strftime-and-strptime-behavior>`_ for more information on choices, though\n",
      "    note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n",
      "    You can also pass:\n",
      "\n",
      "    - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n",
      "        time string (not necessarily in exactly the same format);\n",
      "    - \"mixed\", to infer the format for each element individually. This is risky,\n",
      "        and you should probably use it along with `dayfirst`.\n",
      "\n",
      "    .. versionadded:: 2.0.0\n",
      "dayfirst : bool, default False\n",
      "    DD/MM format dates, international and European format.\n",
      "cache_dates : bool, default True\n",
      "    If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n",
      "    conversion. May produce significant speed-up when parsing duplicate\n",
      "    date strings, especially ones with timezone offsets.\n",
      "\n",
      "iterator : bool, default False\n",
      "    Return ``TextFileReader`` object for iteration or getting chunks with\n",
      "    ``get_chunk()``.\n",
      "chunksize : int, optional\n",
      "    Number of lines to read from the file per chunk. Passing a value will cause the\n",
      "    function to return a ``TextFileReader`` object for iteration.\n",
      "    See the `IO Tools docs\n",
      "    <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
      "    for more information on ``iterator`` and ``chunksize``.\n",
      "\n",
      "{decompression_options}\n",
      "\n",
      "    .. versionchanged:: 1.4.0 Zstandard support.\n",
      "\n",
      "thousands : str (length 1), optional\n",
      "    Character acting as the thousands separator in numerical values.\n",
      "decimal : str (length 1), default '.'\n",
      "    Character to recognize as decimal point (e.g., use ',' for European data).\n",
      "lineterminator : str (length 1), optional\n",
      "    Character used to denote a line break. Only valid with C parser.\n",
      "quotechar : str (length 1), optional\n",
      "    Character used to denote the start and end of a quoted item. Quoted\n",
      "    items can include the ``delimiter`` and it will be ignored.\n",
      "quoting : {{0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, \\\n",
      "3 or csv.QUOTE_NONE}}, default csv.QUOTE_MINIMAL\n",
      "    Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n",
      "    ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n",
      "    characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n",
      "    or ``lineterminator``.\n",
      "doublequote : bool, default True\n",
      "   When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n",
      "   whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n",
      "   field as a single ``quotechar`` element.\n",
      "escapechar : str (length 1), optional\n",
      "    Character used to escape other characters.\n",
      "comment : str (length 1), optional\n",
      "    Character indicating that the remainder of line should not be parsed.\n",
      "    If found at the beginning\n",
      "    of a line, the line will be ignored altogether. This parameter must be a\n",
      "    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "    fully commented lines are ignored by the parameter ``header`` but not by\n",
      "    ``skiprows``. For example, if ``comment='#'``, parsing\n",
      "    ``#empty\\\\na,b,c\\\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n",
      "    treated as the header.\n",
      "encoding : str, optional, default 'utf-8'\n",
      "    Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n",
      "    standard encodings\n",
      "    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
      "\n",
      "encoding_errors : str, optional, default 'strict'\n",
      "    How encoding errors are treated. `List of possible values\n",
      "    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n",
      "\n",
      "    .. versionadded:: 1.3.0\n",
      "\n",
      "dialect : str or csv.Dialect, optional\n",
      "    If provided, this parameter will override values (default or not) for the\n",
      "    following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n",
      "    ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n",
      "    override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n",
      "    documentation for more details.\n",
      "on_bad_lines : {{'error', 'warn', 'skip'}} or Callable, default 'error'\n",
      "    Specifies what to do upon encountering a bad line (a line with too many fields).\n",
      "    Allowed values are :\n",
      "\n",
      "    - ``'error'``, raise an Exception when a bad line is encountered.\n",
      "    - ``'warn'``, raise a warning when a bad line is encountered and skip that line.\n",
      "    - ``'skip'``, skip bad lines without raising or warning when they are encountered.\n",
      "\n",
      "    .. versionadded:: 1.3.0\n",
      "\n",
      "    .. versionadded:: 1.4.0\n",
      "\n",
      "        - Callable, function with signature\n",
      "          ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n",
      "          bad line. ``bad_line`` is a list of strings split by the ``sep``.\n",
      "          If the function returns ``None``, the bad line will be ignored.\n",
      "          If the function returns a new ``list`` of strings with more elements than\n",
      "          expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n",
      "          Only supported when ``engine='python'``\n",
      "\n",
      "    .. versionchanged:: 2.2.0\n",
      "\n",
      "        - Callable, function with signature\n",
      "          as described in `pyarrow documentation\n",
      "          <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n",
      "          #pyarrow.csv.ParseOptions.invalid_row_handler>`_ when ``engine='pyarrow'``\n",
      "\n",
      "delim_whitespace : bool, default False\n",
      "    Specifies whether or not whitespace (e.g. ``' '`` or ``'\\\\t'``) will be\n",
      "    used as the ``sep`` delimiter. Equivalent to setting ``sep='\\\\s+'``. If this option\n",
      "    is set to ``True``, nothing should be passed in for the ``delimiter``\n",
      "    parameter.\n",
      "\n",
      "    .. deprecated:: 2.2.0\n",
      "        Use ``sep=\"\\\\s+\"`` instead.\n",
      "low_memory : bool, default True\n",
      "    Internally process the file in chunks, resulting in lower memory use\n",
      "    while parsing, but possibly mixed type inference.  To ensure no mixed\n",
      "    types either set ``False``, or specify the type with the ``dtype`` parameter.\n",
      "    Note that the entire file is read into a single :class:`~pandas.DataFrame`\n",
      "    regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n",
      "    chunks. (Only valid with C parser).\n",
      "memory_map : bool, default False\n",
      "    If a filepath is provided for ``filepath_or_buffer``, map the file object\n",
      "    directly onto memory and access the data directly from there. Using this\n",
      "    option can improve performance because there is no longer any I/O overhead.\n",
      "float_precision : {{'high', 'legacy', 'round_trip'}}, optional\n",
      "    Specifies which converter the C engine should use for floating-point\n",
      "    values. The options are ``None`` or ``'high'`` for the ordinary converter,\n",
      "    ``'legacy'`` for the original lower precision pandas converter, and\n",
      "    ``'round_trip'`` for the round-trip converter.\n",
      "\n",
      "{storage_options}\n",
      "\n",
      "dtype_backend : {{'numpy_nullable', 'pyarrow'}}, default 'numpy_nullable'\n",
      "    Back-end data type applied to the resultant :class:`DataFrame`\n",
      "    (still experimental). Behaviour is as follows:\n",
      "\n",
      "    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n",
      "      (default).\n",
      "    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n",
      "      DataFrame.\n",
      "\n",
      "    .. versionadded:: 2.0\n",
      "\n",
      "Returns\n",
      "-------\n",
      "DataFrame or TextFileReader\n",
      "    A comma-separated values (csv) file is returned as two-dimensional\n",
      "    data structure with labeled axes.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "{see_also_func_name} : {see_also_func_summary}\n",
      "read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> pd.{func_name}('data.csv')  # doctest: +SKIP\n",
      "\"\"\"\n",
      ")\n",
      "\n",
      "\n",
      "class _C_Parser_Defaults(TypedDict):\n",
      "    delim_whitespace: Literal[False]\n",
      "    na_filter: Literal[True]\n",
      "    low_memory: Literal[True]\n",
      "    memory_map: Literal[False]\n",
      "    float_precision: None\n",
      "\n",
      "\n",
      "_c_parser_defaults: _C_Parser_Defaults = {\n",
      "    \"delim_whitespace\": False,\n",
      "    \"na_filter\": True,\n",
      "    \"low_memory\": True,\n",
      "    \"memory_map\": False,\n",
      "    \"float_precision\": None,\n",
      "}\n",
      "\n",
      "\n",
      "class _Fwf_Defaults(TypedDict):\n",
      "    colspecs: Literal[\"infer\"]\n",
      "    infer_nrows: Literal[100]\n",
      "    widths: None\n",
      "\n",
      "\n",
      "_fwf_defaults: _Fwf_Defaults = {\"colspecs\": \"infer\", \"infer_nrows\": 100, \"widths\": None}\n",
      "_c_unsupported = {\"skipfooter\"}\n",
      "_python_unsupported = {\"low_memory\", \"float_precision\"}\n",
      "_pyarrow_unsupported = {\n",
      "    \"skipfooter\",\n",
      "    \"float_precision\",\n",
      "    \"chunksize\",\n",
      "    \"comment\",\n",
      "    \"nrows\",\n",
      "    \"thousands\",\n",
      "    \"memory_map\",\n",
      "    \"dialect\",\n",
      "    \"delim_whitespace\",\n",
      "    \"quoting\",\n",
      "    \"lineterminator\",\n",
      "    \"converters\",\n",
      "    \"iterator\",\n",
      "    \"dayfirst\",\n",
      "    \"verbose\",\n",
      "    \"skipinitialspace\",\n",
      "    \"low_memory\",\n",
      "}\n",
      "\n",
      "\n",
      "class _DeprecationConfig(NamedTuple):\n",
      "    default_value: Any\n",
      "    msg: str | None\n",
      "\n",
      "\n",
      "@overload\n",
      "def validate_integer(name: str, val: None, min_val: int = ...) -> None:\n",
      "    ...\n",
      "\n",
      "\n",
      "@overload\n",
      "def validate_integer(name: str, val: float, min_val: int = ...) -> int:\n",
      "    ...\n",
      "\n",
      "\n",
      "@overload\n",
      "def validate_integer(name: str, val: int | None, min_val: int = ...) -> int | None:\n",
      "    ...\n",
      "\n",
      "\n",
      "def validate_integer(\n",
      "    name: str, val: int | float | None, min_val: int = 0\n",
      ") -> int | None:\n",
      "    \"\"\"\n",
      "    Checks whether the 'name' parameter for parsing is either\n",
      "    an integer OR float that can SAFELY be cast to an integer\n",
      "    without losing accuracy. Raises a ValueError if that is\n",
      "    not the case.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    name : str\n",
      "        Parameter name (used for error reporting)\n",
      "    val : int or float\n",
      "        The value to check\n",
      "    min_val : int\n",
      "        Minimum allowed value (val < min_val will result in a ValueError)\n",
      "    \"\"\"\n",
      "    if val is None:\n",
      "        return val\n",
      "\n",
      "    msg = f\"'{name:s}' must be an integer >={min_val:d}\"\n",
      "    if is_float(val):\n",
      "        if int(val) != val:\n",
      "            raise ValueError(msg)\n",
      "        val = int(val)\n",
      "    elif not (is_integer(val) and val >= min_val):\n",
      "        raise ValueError(msg)\n",
      "\n",
      "    return int(val)\n",
      "\n",
      "\n",
      "def _validate_names(names: Sequence[Hashable] | None) -> None:\n",
      "    \"\"\"\n",
      "    Raise ValueError if the `names` parameter contains duplicates or has an\n",
      "    invalid data type.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    names : array-like or None\n",
      "        An array containing a list of the names used for the output DataFrame.\n",
      "\n",
      "    Raises\n",
      "    ------\n",
      "    ValueError\n",
      "        If names are not unique or are not ordered (e.g. set).\n",
      "    \"\"\"\n",
      "    if names is not None:\n",
      "        if len(names) != len(set(names)):\n",
      "            raise ValueError(\"Duplicate names are not allowed.\")\n",
      "        if not (\n",
      "            is_list_like(names, allow_sets=False) or isinstance(names, abc.KeysView)\n",
      "        ):\n",
      "            raise ValueError(\"Names should be an ordered collection.\")\n",
      "\n",
      "\n",
      "def _read(\n",
      "    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str], kwds\n",
      ") -> DataFrame | TextFileReader:\n",
      "    \"\"\"Generic reader of line files.\"\"\"\n",
      "    # if we pass a date_parser and parse_dates=False, we should not parse the\n",
      "    # dates GH#44366\n",
      "    if kwds.get(\"parse_dates\", None) is None:\n",
      "        if (\n",
      "            kwds.get(\"date_parser\", lib.no_default) is lib.no_default\n",
      "            and kwds.get(\"date_format\", None) is None\n",
      "        ):\n",
      "            kwds[\"parse_dates\"] = False\n",
      "        else:\n",
      "            kwds[\"parse_dates\"] = True\n",
      "\n",
      "    # Extract some of the arguments (pass chunksize on).\n",
      "    iterator = kwds.get(\"iterator\", False)\n",
      "    chunksize = kwds.get(\"chunksize\", None)\n",
      "    if kwds.get(\"engine\") == \"pyarrow\":\n",
      "        if iterator:\n",
      "            raise ValueError(\n",
      "                \"The 'iterator' option is not supported with the 'pyarrow' engine\"\n",
      "            )\n",
      "\n",
      "        if chunksize is not None:\n",
      "            raise ValueError(\n",
      "                \"The 'chunksize' option is not supported with the 'pyarrow' engine\"\n",
      "            )\n",
      "    else:\n",
      "        chunksize = validate_integer(\"chunksize\", chunksize, 1)\n",
      "\n",
      "    nrows = kwds.get(\"nrows\", None)\n",
      "\n",
      "    # Check for duplicates in names.\n",
      "    _validate_names(kwds.get(\"names\", None))\n",
      "\n",
      "    # Create the parser.\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "\n",
      "    if chunksize or iterator:\n",
      "        return parser\n",
      "\n",
      "    with parser:\n",
      "        return parser.read(nrows)\n",
      "\n",
      "\n",
      "# iterator=True -> TextFileReader\n",
      "@overload\n",
      "def read_csv(\n",
      "    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n",
      "    *,\n",
      "    sep: str | None | lib.NoDefault = ...,\n",
      "    delimiter: str | None | lib.NoDefault = ...,\n",
      "    header: int | Sequence[int] | None | Literal[\"infer\"] = ...,\n",
      "    names: Sequence[Hashable] | None | lib.NoDefault = ...,\n",
      "    index_col: IndexLabel | Literal[False] | None = ...,\n",
      "    usecols: UsecolsArgType = ...,\n",
      "    dtype: DtypeArg | None = ...,\n",
      "    engine: CSVEngine | None = ...,\n",
      "    converters: Mapping[Hashable, Callable] | None = ...,\n",
      "    true_values: list | None = ...,\n",
      "    false_values: list | None = ...,\n",
      "    skipinitialspace: bool = ...,\n",
      "    skiprows: list[int] | int | Callable[[Hashable], bool] | None = ...,\n",
      "    skipfooter: int = ...,\n",
      "    nrows: int | None = ...,\n",
      "    na_values: Hashable\n",
      "    | Iterable[Hashable]\n",
      "    | Mapping[Hashable, Iterable[Hashable]]\n",
      "    | None = ...,\n",
      "    na_filter: bool = ...,\n",
      "    verbose: bool | lib.NoDefault = ...,\n",
      "    skip_blank_lines: bool = ...,\n",
      "    parse_dates: bool | Sequence[Hashable] | None = ...,\n",
      "    infer_datetime_format: bool | lib.NoDefault = ...,\n",
      "    keep_date_col: bool | lib.NoDefault = ...,\n",
      "    date_parser: Callable | lib.NoDefault = ...,\n",
      "    date_format: str | dict[Hashable, str] | None = ...,\n",
      "    dayfirst: bool = ...,\n",
      "    cache_dates: bool = ...,\n",
      "    iterator: Literal[True],\n",
      "    chunksize: int | None = ...,\n",
      "    compression: CompressionOptions = ...,\n",
      "    thousands: str | None = ...,\n",
      "    decimal: str = ...,\n",
      "    lineterminator: str | None = ...,\n",
      "    quotechar: str = ...,\n",
      "    quoting: int = ...,\n",
      "    doublequote: bool = ...,\n",
      "    escapechar: str | None = ...,\n",
      "    comment: str | None = ...,\n",
      "    encoding: str | None = ...,\n",
      "    encoding_errors: str | None = ...,\n",
      "    dialect: str | csv.Dialect | None = ...,\n",
      "    on_bad_lines=...,\n",
      "    delim_whitespace: bool | lib.NoDefault = ...,\n",
      "    low_memory: bool = ...,\n",
      "    memory_map: bool = ...,\n",
      "    float_precision: Literal[\"high\", \"legacy\"] | None = ...,\n",
      "    storage_options: StorageOptions = ...,\n",
      "    dtype_backend: DtypeBackend | lib.NoDefault = ...,\n",
      ") -> TextFileReader:\n",
      "    ...\n",
      "\n",
      "\n",
      "# chunksize=int -> TextFileReader\n",
      "@overload\n",
      "def read_csv(\n",
      "    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n",
      "    *,\n",
      "    sep: str | None | lib.NoDefault = ...,\n",
      "    delimiter: str | None | lib.NoDefault = ...,\n",
      "    header: int | Sequence[int] | None | Literal[\"infer\"] = ...,\n",
      "    names: Sequence[Hashable] | None | lib.NoDefault = ...,\n",
      "    index_col: IndexLabel | Literal[False] | None = ...,\n",
      "    usecols: UsecolsArgType = ...,\n",
      "    dtype: DtypeArg | None = ...,\n",
      "    engine: CSVEngine | None = ...,\n",
      "    converters: Mapping[Hashable, Callable] | None = ...,\n",
      "    true_values: list | None = ...,\n",
      "    false_values: list | None = ...,\n",
      "    skipinitialspace: bool = ...,\n",
      "    skiprows: list[int] | int | Callable[[Hashable], bool] | None = ...,\n",
      "    skipfooter: int = ...,\n",
      "    nrows: int | None = ...,\n",
      "    na_values: Hashable\n",
      "    | Iterable[Hashable]\n",
      "    | Mapping[Hashable, Iterable[Hashable]]\n",
      "    | None = ...,\n",
      "    keep_default_na: bool = ...,\n",
      "    na_filter: bool = ...,\n",
      "    verbose: bool | lib.NoDefault = ...,\n",
      "    skip_blank_lines: bool = ...,\n",
      "    parse_dates: bool | Sequence[Hashable] | None = ...,\n",
      "    infer_datetime_format: bool | lib.NoDefault = ...,\n",
      "    keep_date_col: bool | lib.NoDefault = ...,\n",
      "    date_parser: Callable | lib.NoDefault = ...,\n",
      "    date_format: str | dict[Hashable, str] | None = ...,\n",
      "    dayfirst: bool = ...,\n",
      "    cache_dates: bool = ...,\n",
      "    iterator: bool = ...,\n",
      "    chunksize: int,\n",
      "    compression: CompressionOptions = ...,\n",
      "    thousands: str | None = ...,\n",
      "    decimal: str = ...,\n",
      "    lineterminator: str | None = ...,\n",
      "    quotechar: str = ...,\n",
      "    quoting: int = ...,\n",
      "    doublequote: bool = ...,\n",
      "    escapechar: str | None = ...,\n",
      "    comment: str | None = ...,\n",
      "    encoding: str | None = ...,\n",
      "    encoding_errors: str | None = ...,\n",
      "    dialect: str | csv.Dialect | None = ...,\n",
      "    on_bad_lines=...,\n",
      "    delim_whitespace: bool | lib.NoDefault = ...,\n",
      "    low_memory: bool = ...,\n",
      "    memory_map: bool = ...,\n",
      "    float_precision: Literal[\"high\", \"legacy\"] | None = ...,\n",
      "    storage_options: StorageOptions = ...,\n",
      "    dtype_backend: DtypeBackend | lib.NoDefault = ...,\n",
      ") -> TextFileReader:\n",
      "    ...\n",
      "\n",
      "\n",
      "# default case -> DataFrame\n",
      "@overload\n",
      "def read_csv(\n",
      "    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n",
      "    *,\n",
      "    sep: str | None | lib.NoDefault = ...,\n",
      "    delimiter: str | None | lib.NoDefault = ...,\n",
      "    header: int | Sequence[int] | None | Literal[\"infer\"] = ...,\n",
      "    names: Sequence[Hashable] | None | lib.NoDefault = ...,\n",
      "    index_col: IndexLabel | Literal[False] | None = ...,\n",
      "    usecols: UsecolsArgType = ...,\n",
      "    dtype: DtypeArg | None = ...,\n",
      "    engine: CSVEngine | None = ...,\n",
      "    converters: Mapping[Hashable, Callable] | None = ...,\n",
      "    true_values: list | None = ...,\n",
      "    false_values: list | None = ...,\n",
      "    skipinitialspace: bool = ...,\n",
      "    skiprows: list[int] | int | Callable[[Hashable], bool] | None = ...,\n",
      "    skipfooter: int = ...,\n",
      "    nrows: int | None = ...,\n",
      "    na_values: Hashable\n",
      "    | Iterable[Hashable]\n",
      "    | Mapping[Hashable, Iterable[Hashable]]\n",
      "    | None = ...,\n",
      "    keep_default_na: bool = ...,\n",
      "    na_filter: bool = ...,\n",
      "    verbose: bool | lib.NoDefault = ...,\n",
      "    skip_blank_lines: bool = ...,\n",
      "    parse_dates: bool | Sequence[Hashable] | None = ...,\n",
      "    infer_datetime_format: bool | lib.NoDefault = ...,\n",
      "    keep_date_col: bool | lib.NoDefault = ...,\n",
      "    date_parser: Callable | lib.NoDefault = ...,\n",
      "    date_format: str | dict[Hashable, str] | None = ...,\n",
      "    dayfirst: bool = ...,\n",
      "    cache_dates: bool = ...,\n",
      "    iterator: Literal[False] = ...,\n",
      "    chunksize: None = ...,\n",
      "    compression: CompressionOptions = ...,\n",
      "    thousands: str | None = ...,\n",
      "    decimal: str = ...,\n",
      "    lineterminator: str | None = ...,\n",
      "    quotechar: str = ...,\n",
      "    quoting: int = ...,\n",
      "    doublequote: bool = ...,\n",
      "    escapechar: str | None = ...,\n",
      "    comment: str | None = ...,\n",
      "    encoding: str | None = ...,\n",
      "    encoding_errors: str | None = ...,\n",
      "    dialect: str | csv.Dialect | None = ...,\n",
      "    on_bad_lines=...,\n",
      "    delim_whitespace: bool | lib.NoDefault = ...,\n",
      "    low_memory: bool = ...,\n",
      "    memory_map: bool = ...,\n",
      "    float_precision: Literal[\"high\", \"legacy\"] | None = ...,\n",
      "    storage_options: StorageOptions = ...,\n",
      "    dtype_backend: DtypeBackend | lib.NoDefault = ...,\n",
      ") -> DataFrame:\n",
      "    ...\n",
      "\n",
      "\n",
      "# Unions -> DataFrame | TextFileReader\n",
      "@overload\n",
      "def read_csv(\n",
      "    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n",
      "    *,\n",
      "    sep: str | None | lib.NoDefault = ...,\n",
      "    delimiter: str | None | lib.NoDefault = ...,\n",
      "    header: int | Sequence[int] | None | Literal[\"infer\"] = ...,\n",
      "    names: Sequence[Hashable] | None | lib.NoDefault = ...,\n",
      "    index_col: IndexLabel | Literal[False] | None = ...,\n",
      "    usecols: UsecolsArgType = ...,\n",
      "    dtype: DtypeArg | None = ...,\n",
      "    engine: CSVEngine | None = ...,\n",
      "    converters: Mapping[Hashable, Callable] | None = ...,\n",
      "    true_values: list | None = ...,\n",
      "    false_values: list | None = ...,\n",
      "    skipinitialspace: bool = ...,\n",
      "    skiprows: list[int] | int | Callable[[Hashable], bool] | None = ...,\n",
      "    skipfooter: int = ...,\n",
      "    nrows: int | None = ...,\n",
      "    na_values: Hashable\n",
      "    | Iterable[Hashable]\n",
      "    | Mapping[Hashable, Iterable[Hashable]]\n",
      "    | None = ...,\n",
      "    keep_default_na: bool = ...,\n",
      "    na_filter: bool = ...,\n",
      "    verbose: bool | lib.NoDefault = ...,\n",
      "    skip_blank_lines: bool = ...,\n",
      "    parse_dates: bool | Sequence[Hashable] | None = ...,\n",
      "    infer_datetime_format: bool | lib.NoDefault = ...,\n",
      "    keep_date_col: bool | lib.NoDefault = ...,\n",
      "    date_parser: Callable | lib.NoDefault = ...,\n",
      "    date_format: str | dict[Hashable, str] | None = ...,\n",
      "    dayfirst: bool = ...,\n",
      "    cache_dates: bool = ...,\n",
      "    iterator: bool = ...,\n",
      "    chunksize: int | None = ...,\n",
      "    compression: CompressionOptions = ...,\n",
      "    thousands: str | None = ...,\n",
      "    decimal: str = ...,\n",
      "    lineterminator: str | None = ...,\n",
      "    quotechar: str = ...,\n",
      "    quoting: int = ...,\n",
      "    doublequote: bool = ...,\n",
      "    escapechar: str | None = ...,\n",
      "    comment: str | None = ...,\n",
      "    encoding: str | None = ...,\n",
      "    encoding_errors: str | None = ...,\n",
      "    dialect: str | csv.Dialect | None = ...,\n",
      "    on_bad_lines=...,\n",
      "    delim_whitespace: bool | lib.NoDefault = ...,\n",
      "    low_memory: bool = ...,\n",
      "    memory_map: bool = ...,\n",
      "    float_precision: Literal[\"high\", \"legacy\"] | None = ...,\n",
      "    storage_options: StorageOptions = ...,\n",
      "    dtype_backend: DtypeBackend | lib.NoDefault = ...,\n",
      ") -> DataFrame | TextFileReader:\n",
      "    ...\n",
      "\n",
      "\n",
      "@Appender(\n",
      "    _doc_read_csv_and_table.format(\n",
      "        func_name=\"read_csv\",\n",
      "        summary=\"Read a comma-separated values (csv) file into DataFrame.\",\n",
      "        see_also_func_name=\"read_table\",\n",
      "        see_also_func_summary=\"Read general delimited file into DataFrame.\",\n",
      "        _default_sep=\"','\",\n",
      "        storage_options=_shared_docs[\"storage_options\"],\n",
      "        decompression_options=_shared_docs[\"decompression_options\"]\n",
      "        % \"filepath_or_buffer\",\n",
      "    )\n",
      ")\n",
      "def read_csv(\n",
      "    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n",
      "    *,\n",
      "    sep: str | None | lib.NoDefault = lib.no_default,\n",
      "    delimiter: str | None | lib.NoDefault = None,\n",
      "    # Column and Index Locations and Names\n",
      "    header: int | Sequence[int] | None | Literal[\"infer\"] = \"infer\",\n",
      "    names: Sequence[Hashable] | None | lib.NoDefault = lib.no_default,\n",
      "    index_col: IndexLabel | Literal[False] | None = None,\n",
      "    usecols: UsecolsArgType = None,\n",
      "    # General Parsing Configuration\n",
      "    dtype: DtypeArg | None = None,\n",
      "    engine: CSVEngine | None = None,\n",
      "    converters: Mapping[Hashable, Callable] | None = None,\n",
      "    true_values: list | None = None,\n",
      "    false_values: list | None = None,\n",
      "    skipinitialspace: bool = False,\n",
      "    skiprows: list[int] | int | Callable[[Hashable], bool] | None = None,\n",
      "    skipfooter: int = 0,\n",
      "    nrows: int | None = None,\n",
      "    # NA and Missing Data Handling\n",
      "    na_values: Hashable\n",
      "    | Iterable[Hashable]\n",
      "    | Mapping[Hashable, Iterable[Hashable]]\n",
      "    | None = None,\n",
      "    keep_default_na: bool = True,\n",
      "    na_filter: bool = True,\n",
      "    verbose: bool | lib.NoDefault = lib.no_default,\n",
      "    skip_blank_lines: bool = True,\n",
      "    # Datetime Handling\n",
      "    parse_dates: bool | Sequence[Hashable] | None = None,\n",
      "    infer_datetime_format: bool | lib.NoDefault = lib.no_default,\n",
      "    keep_date_col: bool | lib.NoDefault = lib.no_default,\n",
      "    date_parser: Callable | lib.NoDefault = lib.no_default,\n",
      "    date_format: str | dict[Hashable, str] | None = None,\n",
      "    dayfirst: bool = False,\n",
      "    cache_dates: bool = True,\n",
      "    # Iteration\n",
      "    iterator: bool = False,\n",
      "    chunksize: int | None = None,\n",
      "    # Quoting, Compression, and File Format\n",
      "    compression: CompressionOptions = \"infer\",\n",
      "    thousands: str | None = None,\n",
      "    decimal: str = \".\",\n",
      "    lineterminator: str | None = None,\n",
      "    quotechar: str = '\"',\n",
      "    quoting: int = csv.QUOTE_MINIMAL,\n",
      "    doublequote: bool = True,\n",
      "    escapechar: str | None = None,\n",
      "    comment: str | None = None,\n",
      "    encoding: str | None = None,\n",
      "    encoding_errors: str | None = \"strict\",\n",
      "    dialect: str | csv.Dialect | None = None,\n",
      "    # Error Handling\n",
      "    on_bad_lines: str = \"error\",\n",
      "    # Internal\n",
      "    delim_whitespace: bool | lib.NoDefault = lib.no_default,\n",
      "    low_memory: bool = _c_parser_defaults[\"low_memory\"],\n",
      "    memory_map: bool = False,\n",
      "    float_precision: Literal[\"high\", \"legacy\"] | None = None,\n",
      "    storage_options: StorageOptions | None = None,\n",
      "    dtype_backend: DtypeBackend | lib.NoDefault = lib.no_default,\n",
      ") -> DataFrame | TextFileReader:\n",
      "    if keep_date_col is not lib.no_default:\n",
      "        # GH#55569\n",
      "        warnings.warn(\n",
      "            \"The 'keep_date_col' keyword in pd.read_csv is deprecated and \"\n",
      "            \"will be removed in a future version. Explicitly remove unwanted \"\n",
      "            \"columns after parsing instead.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "    else:\n",
      "        keep_date_col = False\n",
      "\n",
      "    if lib.is_list_like(parse_dates):\n",
      "        # GH#55569\n",
      "        depr = False\n",
      "        # error: Item \"bool\" of \"bool | Sequence[Hashable] | None\" has no\n",
      "        # attribute \"__iter__\" (not iterable)\n",
      "        if not all(is_hashable(x) for x in parse_dates):  # type: ignore[union-attr]\n",
      "            depr = True\n",
      "        elif isinstance(parse_dates, dict) and any(\n",
      "            lib.is_list_like(x) for x in parse_dates.values()\n",
      "        ):\n",
      "            depr = True\n",
      "        if depr:\n",
      "            warnings.warn(\n",
      "                \"Support for nested sequences for 'parse_dates' in pd.read_csv \"\n",
      "                \"is deprecated. Combine the desired columns with pd.to_datetime \"\n",
      "                \"after parsing instead.\",\n",
      "                FutureWarning,\n",
      "                stacklevel=find_stack_level(),\n",
      "            )\n",
      "\n",
      "    if infer_datetime_format is not lib.no_default:\n",
      "        warnings.warn(\n",
      "            \"The argument 'infer_datetime_format' is deprecated and will \"\n",
      "            \"be removed in a future version. \"\n",
      "            \"A strict version of it is now the default, see \"\n",
      "            \"https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. \"\n",
      "            \"You can safely remove this argument.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "\n",
      "    if delim_whitespace is not lib.no_default:\n",
      "        # GH#55569\n",
      "        warnings.warn(\n",
      "            \"The 'delim_whitespace' keyword in pd.read_csv is deprecated and \"\n",
      "            \"will be removed in a future version. Use ``sep='\\\\s+'`` instead\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "    else:\n",
      "        delim_whitespace = False\n",
      "\n",
      "    if verbose is not lib.no_default:\n",
      "        # GH#55569\n",
      "        warnings.warn(\n",
      "            \"The 'verbose' keyword in pd.read_csv is deprecated and \"\n",
      "            \"will be removed in a future version.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "    else:\n",
      "        verbose = False\n",
      "\n",
      "    # locals() should never be modified\n",
      "    kwds = locals().copy()\n",
      "    del kwds[\"filepath_or_buffer\"]\n",
      "    del kwds[\"sep\"]\n",
      "\n",
      "    kwds_defaults = _refine_defaults_read(\n",
      "        dialect,\n",
      "        delimiter,\n",
      "        delim_whitespace,\n",
      "        engine,\n",
      "        sep,\n",
      "        on_bad_lines,\n",
      "        names,\n",
      "        defaults={\"delimiter\": \",\"},\n",
      "        dtype_backend=dtype_backend,\n",
      "    )\n",
      "    kwds.update(kwds_defaults)\n",
      "\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "\n",
      "\n",
      "# iterator=True -> TextFileReader\n",
      "@overload\n",
      "def read_table(\n",
      "    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n",
      "    *,\n",
      "    sep: str | None | lib.NoDefault = ...,\n",
      "    delimiter: str | None | lib.NoDefault = ...,\n",
      "    header: int | Sequence[int] | None | Literal[\"infer\"] = ...,\n",
      "    names: Sequence[Hashable] | None | lib.NoDefault = ...,\n",
      "    index_col: IndexLabel | Literal[False] | None = ...,\n",
      "    usecols: UsecolsArgType = ...,\n",
      "    dtype: DtypeArg | None = ...,\n",
      "    engine: CSVEngine | None = ...,\n",
      "    converters: Mapping[Hashable, Callable] | None = ...,\n",
      "    true_values: list | None = ...,\n",
      "    false_values: list | None = ...,\n",
      "    skipinitialspace: bool = ...,\n",
      "    skiprows: list[int] | int | Callable[[Hashable], bool] | None = ...,\n",
      "    skipfooter: int = ...,\n",
      "    nrows: int | None = ...,\n",
      "    na_values: Sequence[str] | Mapping[str, Sequence[str]] | None = ...,\n",
      "    keep_default_na: bool = ...,\n",
      "    na_filter: bool = ...,\n",
      "    verbose: bool | lib.NoDefault = ...,\n",
      "    skip_blank_lines: bool = ...,\n",
      "    parse_dates: bool | Sequence[Hashable] = ...,\n",
      "    infer_datetime_format: bool | lib.NoDefault = ...,\n",
      "    keep_date_col: bool | lib.NoDefault = ...,\n",
      "    date_parser: Callable | lib.NoDefault = ...,\n",
      "    date_format: str | dict[Hashable, str] | None = ...,\n",
      "    dayfirst: bool = ...,\n",
      "    cache_dates: bool = ...,\n",
      "    iterator: Literal[True],\n",
      "    chunksize: int | None = ...,\n",
      "    compression: CompressionOptions = ...,\n",
      "    thousands: str | None = ...,\n",
      "    decimal: str = ...,\n",
      "    lineterminator: str | None = ...,\n",
      "    quotechar: str = ...,\n",
      "    quoting: int = ...,\n",
      "    doublequote: bool = ...,\n",
      "    escapechar: str | None = ...,\n",
      "    comment: str | None = ...,\n",
      "    encoding: str | None = ...,\n",
      "    encoding_errors: str | None = ...,\n",
      "    dialect: str | csv.Dialect | None = ...,\n",
      "    on_bad_lines=...,\n",
      "    delim_whitespace: bool = ...,\n",
      "    low_memory: bool = ...,\n",
      "    memory_map: bool = ...,\n",
      "    float_precision: str | None = ...,\n",
      "    storage_options: StorageOptions = ...,\n",
      "    dtype_backend: DtypeBackend | lib.NoDefault = ...,\n",
      ") -> TextFileReader:\n",
      "    ...\n",
      "\n",
      "\n",
      "# chunksize=int -> TextFileReader\n",
      "@overload\n",
      "def read_table(\n",
      "    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n",
      "    *,\n",
      "    sep: str | None | lib.NoDefault = ...,\n",
      "    delimiter: str | None | lib.NoDefault = ...,\n",
      "    header: int | Sequence[int] | None | Literal[\"infer\"] = ...,\n",
      "    names: Sequence[Hashable] | None | lib.NoDefault = ...,\n",
      "    index_col: IndexLabel | Literal[False] | None = ...,\n",
      "    usecols: UsecolsArgType = ...,\n",
      "    dtype: DtypeArg | None = ...,\n",
      "    engine: CSVEngine | None = ...,\n",
      "    converters: Mapping[Hashable, Callable] | None = ...,\n",
      "    true_values: list | None = ...,\n",
      "    false_values: list | None = ...,\n",
      "    skipinitialspace: bool = ...,\n",
      "    skiprows: list[int] | int | Callable[[Hashable], bool] | None = ...,\n",
      "    skipfooter: int = ...,\n",
      "    nrows: int | None = ...,\n",
      "    na_values: Sequence[str] | Mapping[str, Sequence[str]] | None = ...,\n",
      "    keep_default_na: bool = ...,\n",
      "    na_filter: bool = ...,\n",
      "    verbose: bool | lib.NoDefault = ...,\n",
      "    skip_blank_lines: bool = ...,\n",
      "    parse_dates: bool | Sequence[Hashable] = ...,\n",
      "    infer_datetime_format: bool | lib.NoDefault = ...,\n",
      "    keep_date_col: bool | lib.NoDefault = ...,\n",
      "    date_parser: Callable | lib.NoDefault = ...,\n",
      "    date_format: str | dict[Hashable, str] | None = ...,\n",
      "    dayfirst: bool = ...,\n",
      "    cache_dates: bool = ...,\n",
      "    iterator: bool = ...,\n",
      "    chunksize: int,\n",
      "    compression: CompressionOptions = ...,\n",
      "    thousands: str | None = ...,\n",
      "    decimal: str = ...,\n",
      "    lineterminator: str | None = ...,\n",
      "    quotechar: str = ...,\n",
      "    quoting: int = ...,\n",
      "    doublequote: bool = ...,\n",
      "    escapechar: str | None = ...,\n",
      "    comment: str | None = ...,\n",
      "    encoding: str | None = ...,\n",
      "    encoding_errors: str | None = ...,\n",
      "    dialect: str | csv.Dialect | None = ...,\n",
      "    on_bad_lines=...,\n",
      "    delim_whitespace: bool = ...,\n",
      "    low_memory: bool = ...,\n",
      "    memory_map: bool = ...,\n",
      "    float_precision: str | None = ...,\n",
      "    storage_options: StorageOptions = ...,\n",
      "    dtype_backend: DtypeBackend | lib.NoDefault = ...,\n",
      ") -> TextFileReader:\n",
      "    ...\n",
      "\n",
      "\n",
      "# default -> DataFrame\n",
      "@overload\n",
      "def read_table(\n",
      "    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n",
      "    *,\n",
      "    sep: str | None | lib.NoDefault = ...,\n",
      "    delimiter: str | None | lib.NoDefault = ...,\n",
      "    header: int | Sequence[int] | None | Literal[\"infer\"] = ...,\n",
      "    names: Sequence[Hashable] | None | lib.NoDefault = ...,\n",
      "    index_col: IndexLabel | Literal[False] | None = ...,\n",
      "    usecols: UsecolsArgType = ...,\n",
      "    dtype: DtypeArg | None = ...,\n",
      "    engine: CSVEngine | None = ...,\n",
      "    converters: Mapping[Hashable, Callable] | None = ...,\n",
      "    true_values: list | None = ...,\n",
      "    false_values: list | None = ...,\n",
      "    skipinitialspace: bool = ...,\n",
      "    skiprows: list[int] | int | Callable[[Hashable], bool] | None = ...,\n",
      "    skipfooter: int = ...,\n",
      "    nrows: int | None = ...,\n",
      "    na_values: Sequence[str] | Mapping[str, Sequence[str]] | None = ...,\n",
      "    keep_default_na: bool = ...,\n",
      "    na_filter: bool = ...,\n",
      "    verbose: bool | lib.NoDefault = ...,\n",
      "    skip_blank_lines: bool = ...,\n",
      "    parse_dates: bool | Sequence[Hashable] = ...,\n",
      "    infer_datetime_format: bool | lib.NoDefault = ...,\n",
      "    keep_date_col: bool | lib.NoDefault = ...,\n",
      "    date_parser: Callable | lib.NoDefault = ...,\n",
      "    date_format: str | dict[Hashable, str] | None = ...,\n",
      "    dayfirst: bool = ...,\n",
      "    cache_dates: bool = ...,\n",
      "    iterator: Literal[False] = ...,\n",
      "    chunksize: None = ...,\n",
      "    compression: CompressionOptions = ...,\n",
      "    thousands: str | None = ...,\n",
      "    decimal: str = ...,\n",
      "    lineterminator: str | None = ...,\n",
      "    quotechar: str = ...,\n",
      "    quoting: int = ...,\n",
      "    doublequote: bool = ...,\n",
      "    escapechar: str | None = ...,\n",
      "    comment: str | None = ...,\n",
      "    encoding: str | None = ...,\n",
      "    encoding_errors: str | None = ...,\n",
      "    dialect: str | csv.Dialect | None = ...,\n",
      "    on_bad_lines=...,\n",
      "    delim_whitespace: bool = ...,\n",
      "    low_memory: bool = ...,\n",
      "    memory_map: bool = ...,\n",
      "    float_precision: str | None = ...,\n",
      "    storage_options: StorageOptions = ...,\n",
      "    dtype_backend: DtypeBackend | lib.NoDefault = ...,\n",
      ") -> DataFrame:\n",
      "    ...\n",
      "\n",
      "\n",
      "# Unions -> DataFrame | TextFileReader\n",
      "@overload\n",
      "def read_table(\n",
      "    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n",
      "    *,\n",
      "    sep: str | None | lib.NoDefault = ...,\n",
      "    delimiter: str | None | lib.NoDefault = ...,\n",
      "    header: int | Sequence[int] | None | Literal[\"infer\"] = ...,\n",
      "    names: Sequence[Hashable] | None | lib.NoDefault = ...,\n",
      "    index_col: IndexLabel | Literal[False] | None = ...,\n",
      "    usecols: UsecolsArgType = ...,\n",
      "    dtype: DtypeArg | None = ...,\n",
      "    engine: CSVEngine | None = ...,\n",
      "    converters: Mapping[Hashable, Callable] | None = ...,\n",
      "    true_values: list | None = ...,\n",
      "    false_values: list | None = ...,\n",
      "    skipinitialspace: bool = ...,\n",
      "    skiprows: list[int] | int | Callable[[Hashable], bool] | None = ...,\n",
      "    skipfooter: int = ...,\n",
      "    nrows: int | None = ...,\n",
      "    na_values: Sequence[str] | Mapping[str, Sequence[str]] | None = ...,\n",
      "    keep_default_na: bool = ...,\n",
      "    na_filter: bool = ...,\n",
      "    verbose: bool | lib.NoDefault = ...,\n",
      "    skip_blank_lines: bool = ...,\n",
      "    parse_dates: bool | Sequence[Hashable] = ...,\n",
      "    infer_datetime_format: bool | lib.NoDefault = ...,\n",
      "    keep_date_col: bool | lib.NoDefault = ...,\n",
      "    date_parser: Callable | lib.NoDefault = ...,\n",
      "    date_format: str | dict[Hashable, str] | None = ...,\n",
      "    dayfirst: bool = ...,\n",
      "    cache_dates: bool = ...,\n",
      "    iterator: bool = ...,\n",
      "    chunksize: int | None = ...,\n",
      "    compression: CompressionOptions = ...,\n",
      "    thousands: str | None = ...,\n",
      "    decimal: str = ...,\n",
      "    lineterminator: str | None = ...,\n",
      "    quotechar: str = ...,\n",
      "    quoting: int = ...,\n",
      "    doublequote: bool = ...,\n",
      "    escapechar: str | None = ...,\n",
      "    comment: str | None = ...,\n",
      "    encoding: str | None = ...,\n",
      "    encoding_errors: str | None = ...,\n",
      "    dialect: str | csv.Dialect | None = ...,\n",
      "    on_bad_lines=...,\n",
      "    delim_whitespace: bool = ...,\n",
      "    low_memory: bool = ...,\n",
      "    memory_map: bool = ...,\n",
      "    float_precision: str | None = ...,\n",
      "    storage_options: StorageOptions = ...,\n",
      "    dtype_backend: DtypeBackend | lib.NoDefault = ...,\n",
      ") -> DataFrame | TextFileReader:\n",
      "    ...\n",
      "\n",
      "\n",
      "@Appender(\n",
      "    _doc_read_csv_and_table.format(\n",
      "        func_name=\"read_table\",\n",
      "        summary=\"Read general delimited file into DataFrame.\",\n",
      "        see_also_func_name=\"read_csv\",\n",
      "        see_also_func_summary=(\n",
      "            \"Read a comma-separated values (csv) file into DataFrame.\"\n",
      "        ),\n",
      "        _default_sep=r\"'\\\\t' (tab-stop)\",\n",
      "        storage_options=_shared_docs[\"storage_options\"],\n",
      "        decompression_options=_shared_docs[\"decompression_options\"]\n",
      "        % \"filepath_or_buffer\",\n",
      "    )\n",
      ")\n",
      "def read_table(\n",
      "    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n",
      "    *,\n",
      "    sep: str | None | lib.NoDefault = lib.no_default,\n",
      "    delimiter: str | None | lib.NoDefault = None,\n",
      "    # Column and Index Locations and Names\n",
      "    header: int | Sequence[int] | None | Literal[\"infer\"] = \"infer\",\n",
      "    names: Sequence[Hashable] | None | lib.NoDefault = lib.no_default,\n",
      "    index_col: IndexLabel | Literal[False] | None = None,\n",
      "    usecols: UsecolsArgType = None,\n",
      "    # General Parsing Configuration\n",
      "    dtype: DtypeArg | None = None,\n",
      "    engine: CSVEngine | None = None,\n",
      "    converters: Mapping[Hashable, Callable] | None = None,\n",
      "    true_values: list | None = None,\n",
      "    false_values: list | None = None,\n",
      "    skipinitialspace: bool = False,\n",
      "    skiprows: list[int] | int | Callable[[Hashable], bool] | None = None,\n",
      "    skipfooter: int = 0,\n",
      "    nrows: int | None = None,\n",
      "    # NA and Missing Data Handling\n",
      "    na_values: Sequence[str] | Mapping[str, Sequence[str]] | None = None,\n",
      "    keep_default_na: bool = True,\n",
      "    na_filter: bool = True,\n",
      "    verbose: bool | lib.NoDefault = lib.no_default,\n",
      "    skip_blank_lines: bool = True,\n",
      "    # Datetime Handling\n",
      "    parse_dates: bool | Sequence[Hashable] = False,\n",
      "    infer_datetime_format: bool | lib.NoDefault = lib.no_default,\n",
      "    keep_date_col: bool | lib.NoDefault = lib.no_default,\n",
      "    date_parser: Callable | lib.NoDefault = lib.no_default,\n",
      "    date_format: str | dict[Hashable, str] | None = None,\n",
      "    dayfirst: bool = False,\n",
      "    cache_dates: bool = True,\n",
      "    # Iteration\n",
      "    iterator: bool = False,\n",
      "    chunksize: int | None = None,\n",
      "    # Quoting, Compression, and File Format\n",
      "    compression: CompressionOptions = \"infer\",\n",
      "    thousands: str | None = None,\n",
      "    decimal: str = \".\",\n",
      "    lineterminator: str | None = None,\n",
      "    quotechar: str = '\"',\n",
      "    quoting: int = csv.QUOTE_MINIMAL,\n",
      "    doublequote: bool = True,\n",
      "    escapechar: str | None = None,\n",
      "    comment: str | None = None,\n",
      "    encoding: str | None = None,\n",
      "    encoding_errors: str | None = \"strict\",\n",
      "    dialect: str | csv.Dialect | None = None,\n",
      "    # Error Handling\n",
      "    on_bad_lines: str = \"error\",\n",
      "    # Internal\n",
      "    delim_whitespace: bool | lib.NoDefault = lib.no_default,\n",
      "    low_memory: bool = _c_parser_defaults[\"low_memory\"],\n",
      "    memory_map: bool = False,\n",
      "    float_precision: str | None = None,\n",
      "    storage_options: StorageOptions | None = None,\n",
      "    dtype_backend: DtypeBackend | lib.NoDefault = lib.no_default,\n",
      ") -> DataFrame | TextFileReader:\n",
      "    if keep_date_col is not lib.no_default:\n",
      "        # GH#55569\n",
      "        warnings.warn(\n",
      "            \"The 'keep_date_col' keyword in pd.read_table is deprecated and \"\n",
      "            \"will be removed in a future version. Explicitly remove unwanted \"\n",
      "            \"columns after parsing instead.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "    else:\n",
      "        keep_date_col = False\n",
      "\n",
      "    # error: Item \"bool\" of \"bool | Sequence[Hashable]\" has no attribute \"__iter__\"\n",
      "    if lib.is_list_like(parse_dates) and not all(is_hashable(x) for x in parse_dates):  # type: ignore[union-attr]\n",
      "        # GH#55569\n",
      "        warnings.warn(\n",
      "            \"Support for nested sequences for 'parse_dates' in pd.read_table \"\n",
      "            \"is deprecated. Combine the desired columns with pd.to_datetime \"\n",
      "            \"after parsing instead.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "\n",
      "    if infer_datetime_format is not lib.no_default:\n",
      "        warnings.warn(\n",
      "            \"The argument 'infer_datetime_format' is deprecated and will \"\n",
      "            \"be removed in a future version. \"\n",
      "            \"A strict version of it is now the default, see \"\n",
      "            \"https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. \"\n",
      "            \"You can safely remove this argument.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "\n",
      "    if delim_whitespace is not lib.no_default:\n",
      "        # GH#55569\n",
      "        warnings.warn(\n",
      "            \"The 'delim_whitespace' keyword in pd.read_table is deprecated and \"\n",
      "            \"will be removed in a future version. Use ``sep='\\\\s+'`` instead\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "    else:\n",
      "        delim_whitespace = False\n",
      "\n",
      "    if verbose is not lib.no_default:\n",
      "        # GH#55569\n",
      "        warnings.warn(\n",
      "            \"The 'verbose' keyword in pd.read_table is deprecated and \"\n",
      "            \"will be removed in a future version.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "    else:\n",
      "        verbose = False\n",
      "\n",
      "    # locals() should never be modified\n",
      "    kwds = locals().copy()\n",
      "    del kwds[\"filepath_or_buffer\"]\n",
      "    del kwds[\"sep\"]\n",
      "\n",
      "    kwds_defaults = _refine_defaults_read(\n",
      "        dialect,\n",
      "        delimiter,\n",
      "        delim_whitespace,\n",
      "        engine,\n",
      "        sep,\n",
      "        on_bad_lines,\n",
      "        names,\n",
      "        defaults={\"delimiter\": \"\\t\"},\n",
      "        dtype_backend=dtype_backend,\n",
      "    )\n",
      "    kwds.update(kwds_defaults)\n",
      "\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "\n",
      "\n",
      "@overload\n",
      "def read_fwf(\n",
      "    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n",
      "    *,\n",
      "    colspecs: Sequence[tuple[int, int]] | str | None = ...,\n",
      "    widths: Sequence[int] | None = ...,\n",
      "    infer_nrows: int = ...,\n",
      "    dtype_backend: DtypeBackend | lib.NoDefault = ...,\n",
      "    iterator: Literal[True],\n",
      "    chunksize: int | None = ...,\n",
      "    **kwds,\n",
      ") -> TextFileReader:\n",
      "    ...\n",
      "\n",
      "\n",
      "@overload\n",
      "def read_fwf(\n",
      "    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n",
      "    *,\n",
      "    colspecs: Sequence[tuple[int, int]] | str | None = ...,\n",
      "    widths: Sequence[int] | None = ...,\n",
      "    infer_nrows: int = ...,\n",
      "    dtype_backend: DtypeBackend | lib.NoDefault = ...,\n",
      "    iterator: bool = ...,\n",
      "    chunksize: int,\n",
      "    **kwds,\n",
      ") -> TextFileReader:\n",
      "    ...\n",
      "\n",
      "\n",
      "@overload\n",
      "def read_fwf(\n",
      "    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n",
      "    *,\n",
      "    colspecs: Sequence[tuple[int, int]] | str | None = ...,\n",
      "    widths: Sequence[int] | None = ...,\n",
      "    infer_nrows: int = ...,\n",
      "    dtype_backend: DtypeBackend | lib.NoDefault = ...,\n",
      "    iterator: Literal[False] = ...,\n",
      "    chunksize: None = ...,\n",
      "    **kwds,\n",
      ") -> DataFrame:\n",
      "    ...\n",
      "\n",
      "\n",
      "def read_fwf(\n",
      "    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n",
      "    *,\n",
      "    colspecs: Sequence[tuple[int, int]] | str | None = \"infer\",\n",
      "    widths: Sequence[int] | None = None,\n",
      "    infer_nrows: int = 100,\n",
      "    dtype_backend: DtypeBackend | lib.NoDefault = lib.no_default,\n",
      "    iterator: bool = False,\n",
      "    chunksize: int | None = None,\n",
      "    **kwds,\n",
      ") -> DataFrame | TextFileReader:\n",
      "    r\"\"\"\n",
      "    Read a table of fixed-width formatted lines into DataFrame.\n",
      "\n",
      "    Also supports optionally iterating or breaking of the file\n",
      "    into chunks.\n",
      "\n",
      "    Additional help can be found in the `online docs for IO Tools\n",
      "    <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    filepath_or_buffer : str, path object, or file-like object\n",
      "        String, path object (implementing ``os.PathLike[str]``), or file-like\n",
      "        object implementing a text ``read()`` function.The string could be a URL.\n",
      "        Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is\n",
      "        expected. A local file could be:\n",
      "        ``file://localhost/path/to/table.csv``.\n",
      "    colspecs : list of tuple (int, int) or 'infer'. optional\n",
      "        A list of tuples giving the extents of the fixed-width\n",
      "        fields of each line as half-open intervals (i.e.,  [from, to[ ).\n",
      "        String value 'infer' can be used to instruct the parser to try\n",
      "        detecting the column specifications from the first 100 rows of\n",
      "        the data which are not being skipped via skiprows (default='infer').\n",
      "    widths : list of int, optional\n",
      "        A list of field widths which can be used instead of 'colspecs' if\n",
      "        the intervals are contiguous.\n",
      "    infer_nrows : int, default 100\n",
      "        The number of rows to consider when letting the parser determine the\n",
      "        `colspecs`.\n",
      "    dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n",
      "        Back-end data type applied to the resultant :class:`DataFrame`\n",
      "        (still experimental). Behaviour is as follows:\n",
      "\n",
      "        * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n",
      "          (default).\n",
      "        * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n",
      "          DataFrame.\n",
      "\n",
      "        .. versionadded:: 2.0\n",
      "\n",
      "    **kwds : optional\n",
      "        Optional keyword arguments can be passed to ``TextFileReader``.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or TextFileReader\n",
      "        A comma-separated values (csv) file is returned as two-dimensional\n",
      "        data structure with labeled axes.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "    read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> pd.read_fwf('data.csv')  # doctest: +SKIP\n",
      "    \"\"\"\n",
      "    # Check input arguments.\n",
      "    if colspecs is None and widths is None:\n",
      "        raise ValueError(\"Must specify either colspecs or widths\")\n",
      "    if colspecs not in (None, \"infer\") and widths is not None:\n",
      "        raise ValueError(\"You must specify only one of 'widths' and 'colspecs'\")\n",
      "\n",
      "    # Compute 'colspecs' from 'widths', if specified.\n",
      "    if widths is not None:\n",
      "        colspecs, col = [], 0\n",
      "        for w in widths:\n",
      "            colspecs.append((col, col + w))\n",
      "            col += w\n",
      "\n",
      "    # for mypy\n",
      "    assert colspecs is not None\n",
      "\n",
      "    # GH#40830\n",
      "    # Ensure length of `colspecs` matches length of `names`\n",
      "    names = kwds.get(\"names\")\n",
      "    if names is not None:\n",
      "        if len(names) != len(colspecs) and colspecs != \"infer\":\n",
      "            # need to check len(index_col) as it might contain\n",
      "            # unnamed indices, in which case it's name is not required\n",
      "            len_index = 0\n",
      "            if kwds.get(\"index_col\") is not None:\n",
      "                index_col: Any = kwds.get(\"index_col\")\n",
      "                if index_col is not False:\n",
      "                    if not is_list_like(index_col):\n",
      "                        len_index = 1\n",
      "                    else:\n",
      "                        len_index = len(index_col)\n",
      "            if kwds.get(\"usecols\") is None and len(names) + len_index != len(colspecs):\n",
      "                # If usecols is used colspec may be longer than names\n",
      "                raise ValueError(\"Length of colspecs must match length of names\")\n",
      "\n",
      "    kwds[\"colspecs\"] = colspecs\n",
      "    kwds[\"infer_nrows\"] = infer_nrows\n",
      "    kwds[\"engine\"] = \"python-fwf\"\n",
      "    kwds[\"iterator\"] = iterator\n",
      "    kwds[\"chunksize\"] = chunksize\n",
      "\n",
      "    check_dtype_backend(dtype_backend)\n",
      "    kwds[\"dtype_backend\"] = dtype_backend\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "\n",
      "\n",
      "class TextFileReader(abc.Iterator):\n",
      "    \"\"\"\n",
      "\n",
      "    Passed dialect overrides any of the related parser options\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        f: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str] | list,\n",
      "        engine: CSVEngine | None = None,\n",
      "        **kwds,\n",
      "    ) -> None:\n",
      "        if engine is not None:\n",
      "            engine_specified = True\n",
      "        else:\n",
      "            engine = \"python\"\n",
      "            engine_specified = False\n",
      "        self.engine = engine\n",
      "        self._engine_specified = kwds.get(\"engine_specified\", engine_specified)\n",
      "\n",
      "        _validate_skipfooter(kwds)\n",
      "\n",
      "        dialect = _extract_dialect(kwds)\n",
      "        if dialect is not None:\n",
      "            if engine == \"pyarrow\":\n",
      "                raise ValueError(\n",
      "                    \"The 'dialect' option is not supported with the 'pyarrow' engine\"\n",
      "                )\n",
      "            kwds = _merge_with_dialect_properties(dialect, kwds)\n",
      "\n",
      "        if kwds.get(\"header\", \"infer\") == \"infer\":\n",
      "            kwds[\"header\"] = 0 if kwds.get(\"names\") is None else None\n",
      "\n",
      "        self.orig_options = kwds\n",
      "\n",
      "        # miscellanea\n",
      "        self._currow = 0\n",
      "\n",
      "        options = self._get_options_with_defaults(engine)\n",
      "        options[\"storage_options\"] = kwds.get(\"storage_options\", None)\n",
      "\n",
      "        self.chunksize = options.pop(\"chunksize\", None)\n",
      "        self.nrows = options.pop(\"nrows\", None)\n",
      "\n",
      "        self._check_file_or_buffer(f, engine)\n",
      "        self.options, self.engine = self._clean_options(options, engine)\n",
      "\n",
      "        if \"has_index_names\" in kwds:\n",
      "            self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n",
      "\n",
      "        self.handles: IOHandles | None = None\n",
      "        self._engine = self._make_engine(f, self.engine)\n",
      "\n",
      "    def close(self) -> None:\n",
      "        if self.handles is not None:\n",
      "            self.handles.close()\n",
      "        self._engine.close()\n",
      "\n",
      "    def _get_options_with_defaults(self, engine: CSVEngine) -> dict[str, Any]:\n",
      "        kwds = self.orig_options\n",
      "\n",
      "        options = {}\n",
      "        default: object | None\n",
      "\n",
      "        for argname, default in parser_defaults.items():\n",
      "            value = kwds.get(argname, default)\n",
      "\n",
      "            # see gh-12935\n",
      "            if (\n",
      "                engine == \"pyarrow\"\n",
      "                and argname in _pyarrow_unsupported\n",
      "                and value != default\n",
      "                and value != getattr(value, \"value\", default)\n",
      "            ):\n",
      "                raise ValueError(\n",
      "                    f\"The {repr(argname)} option is not supported with the \"\n",
      "                    f\"'pyarrow' engine\"\n",
      "                )\n",
      "            options[argname] = value\n",
      "\n",
      "        for argname, default in _c_parser_defaults.items():\n",
      "            if argname in kwds:\n",
      "                value = kwds[argname]\n",
      "\n",
      "                if engine != \"c\" and value != default:\n",
      "                    # TODO: Refactor this logic, its pretty convoluted\n",
      "                    if \"python\" in engine and argname not in _python_unsupported:\n",
      "                        pass\n",
      "                    elif \"pyarrow\" in engine and argname not in _pyarrow_unsupported:\n",
      "                        pass\n",
      "                    else:\n",
      "                        raise ValueError(\n",
      "                            f\"The {repr(argname)} option is not supported with the \"\n",
      "                            f\"{repr(engine)} engine\"\n",
      "                        )\n",
      "            else:\n",
      "                value = default\n",
      "            options[argname] = value\n",
      "\n",
      "        if engine == \"python-fwf\":\n",
      "            for argname, default in _fwf_defaults.items():\n",
      "                options[argname] = kwds.get(argname, default)\n",
      "\n",
      "        return options\n",
      "\n",
      "    def _check_file_or_buffer(self, f, engine: CSVEngine) -> None:\n",
      "        # see gh-16530\n",
      "        if is_file_like(f) and engine != \"c\" and not hasattr(f, \"__iter__\"):\n",
      "            # The C engine doesn't need the file-like to have the \"__iter__\"\n",
      "            # attribute. However, the Python engine needs \"__iter__(...)\"\n",
      "            # when iterating through such an object, meaning it\n",
      "            # needs to have that attribute\n",
      "            raise ValueError(\n",
      "                \"The 'python' engine cannot iterate through this file buffer.\"\n",
      "            )\n",
      "\n",
      "    def _clean_options(\n",
      "        self, options: dict[str, Any], engine: CSVEngine\n",
      "    ) -> tuple[dict[str, Any], CSVEngine]:\n",
      "        result = options.copy()\n",
      "\n",
      "        fallback_reason = None\n",
      "\n",
      "        # C engine not supported yet\n",
      "        if engine == \"c\":\n",
      "            if options[\"skipfooter\"] > 0:\n",
      "                fallback_reason = \"the 'c' engine does not support skipfooter\"\n",
      "                engine = \"python\"\n",
      "\n",
      "        sep = options[\"delimiter\"]\n",
      "        delim_whitespace = options[\"delim_whitespace\"]\n",
      "\n",
      "        if sep is None and not delim_whitespace:\n",
      "            if engine in (\"c\", \"pyarrow\"):\n",
      "                fallback_reason = (\n",
      "                    f\"the '{engine}' engine does not support \"\n",
      "                    \"sep=None with delim_whitespace=False\"\n",
      "                )\n",
      "                engine = \"python\"\n",
      "        elif sep is not None and len(sep) > 1:\n",
      "            if engine == \"c\" and sep == r\"\\s+\":\n",
      "                result[\"delim_whitespace\"] = True\n",
      "                del result[\"delimiter\"]\n",
      "            elif engine not in (\"python\", \"python-fwf\"):\n",
      "                # wait until regex engine integrated\n",
      "                fallback_reason = (\n",
      "                    f\"the '{engine}' engine does not support \"\n",
      "                    \"regex separators (separators > 1 char and \"\n",
      "                    r\"different from '\\s+' are interpreted as regex)\"\n",
      "                )\n",
      "                engine = \"python\"\n",
      "        elif delim_whitespace:\n",
      "            if \"python\" in engine:\n",
      "                result[\"delimiter\"] = r\"\\s+\"\n",
      "        elif sep is not None:\n",
      "            encodeable = True\n",
      "            encoding = sys.getfilesystemencoding() or \"utf-8\"\n",
      "            try:\n",
      "                if len(sep.encode(encoding)) > 1:\n",
      "                    encodeable = False\n",
      "            except UnicodeDecodeError:\n",
      "                encodeable = False\n",
      "            if not encodeable and engine not in (\"python\", \"python-fwf\"):\n",
      "                fallback_reason = (\n",
      "                    f\"the separator encoded in {encoding} \"\n",
      "                    f\"is > 1 char long, and the '{engine}' engine \"\n",
      "                    \"does not support such separators\"\n",
      "                )\n",
      "                engine = \"python\"\n",
      "\n",
      "        quotechar = options[\"quotechar\"]\n",
      "        if quotechar is not None and isinstance(quotechar, (str, bytes)):\n",
      "            if (\n",
      "                len(quotechar) == 1\n",
      "                and ord(quotechar) > 127\n",
      "                and engine not in (\"python\", \"python-fwf\")\n",
      "            ):\n",
      "                fallback_reason = (\n",
      "                    \"ord(quotechar) > 127, meaning the \"\n",
      "                    \"quotechar is larger than one byte, \"\n",
      "                    f\"and the '{engine}' engine does not support such quotechars\"\n",
      "                )\n",
      "                engine = \"python\"\n",
      "\n",
      "        if fallback_reason and self._engine_specified:\n",
      "            raise ValueError(fallback_reason)\n",
      "\n",
      "        if engine == \"c\":\n",
      "            for arg in _c_unsupported:\n",
      "                del result[arg]\n",
      "\n",
      "        if \"python\" in engine:\n",
      "            for arg in _python_unsupported:\n",
      "                if fallback_reason and result[arg] != _c_parser_defaults.get(arg):\n",
      "                    raise ValueError(\n",
      "                        \"Falling back to the 'python' engine because \"\n",
      "                        f\"{fallback_reason}, but this causes {repr(arg)} to be \"\n",
      "                        \"ignored as it is not supported by the 'python' engine.\"\n",
      "                    )\n",
      "                del result[arg]\n",
      "\n",
      "        if fallback_reason:\n",
      "            warnings.warn(\n",
      "                (\n",
      "                    \"Falling back to the 'python' engine because \"\n",
      "                    f\"{fallback_reason}; you can avoid this warning by specifying \"\n",
      "                    \"engine='python'.\"\n",
      "                ),\n",
      "                ParserWarning,\n",
      "                stacklevel=find_stack_level(),\n",
      "            )\n",
      "\n",
      "        index_col = options[\"index_col\"]\n",
      "        names = options[\"names\"]\n",
      "        converters = options[\"converters\"]\n",
      "        na_values = options[\"na_values\"]\n",
      "        skiprows = options[\"skiprows\"]\n",
      "\n",
      "        validate_header_arg(options[\"header\"])\n",
      "\n",
      "        if index_col is True:\n",
      "            raise ValueError(\"The value of index_col couldn't be 'True'\")\n",
      "        if is_index_col(index_col):\n",
      "            if not isinstance(index_col, (list, tuple, np.ndarray)):\n",
      "                index_col = [index_col]\n",
      "        result[\"index_col\"] = index_col\n",
      "\n",
      "        names = list(names) if names is not None else names\n",
      "\n",
      "        # type conversion-related\n",
      "        if converters is not None:\n",
      "            if not isinstance(converters, dict):\n",
      "                raise TypeError(\n",
      "                    \"Type converters must be a dict or subclass, \"\n",
      "                    f\"input was a {type(converters).__name__}\"\n",
      "                )\n",
      "        else:\n",
      "            converters = {}\n",
      "\n",
      "        # Converting values to NA\n",
      "        keep_default_na = options[\"keep_default_na\"]\n",
      "        floatify = engine != \"pyarrow\"\n",
      "        na_values, na_fvalues = _clean_na_values(\n",
      "            na_values, keep_default_na, floatify=floatify\n",
      "        )\n",
      "\n",
      "        # handle skiprows; this is internally handled by the\n",
      "        # c-engine, so only need for python and pyarrow parsers\n",
      "        if engine == \"pyarrow\":\n",
      "            if not is_integer(skiprows) and skiprows is not None:\n",
      "                # pyarrow expects skiprows to be passed as an integer\n",
      "                raise ValueError(\n",
      "                    \"skiprows argument must be an integer when using \"\n",
      "                    \"engine='pyarrow'\"\n",
      "                )\n",
      "        else:\n",
      "            if is_integer(skiprows):\n",
      "                skiprows = list(range(skiprows))\n",
      "            if skiprows is None:\n",
      "                skiprows = set()\n",
      "            elif not callable(skiprows):\n",
      "                skiprows = set(skiprows)\n",
      "\n",
      "        # put stuff back\n",
      "        result[\"names\"] = names\n",
      "        result[\"converters\"] = converters\n",
      "        result[\"na_values\"] = na_values\n",
      "        result[\"na_fvalues\"] = na_fvalues\n",
      "        result[\"skiprows\"] = skiprows\n",
      "\n",
      "        return result, engine\n",
      "\n",
      "    def __next__(self) -> DataFrame:\n",
      "        try:\n",
      "            return self.get_chunk()\n",
      "        except StopIteration:\n",
      "            self.close()\n",
      "            raise\n",
      "\n",
      "    def _make_engine(\n",
      "        self,\n",
      "        f: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str] | list | IO,\n",
      "        engine: CSVEngine = \"c\",\n",
      "    ) -> ParserBase:\n",
      "        mapping: dict[str, type[ParserBase]] = {\n",
      "            \"c\": CParserWrapper,\n",
      "            \"python\": PythonParser,\n",
      "            \"pyarrow\": ArrowParserWrapper,\n",
      "            \"python-fwf\": FixedWidthFieldParser,\n",
      "        }\n",
      "        if engine not in mapping:\n",
      "            raise ValueError(\n",
      "                f\"Unknown engine: {engine} (valid options are {mapping.keys()})\"\n",
      "            )\n",
      "        if not isinstance(f, list):\n",
      "            # open file here\n",
      "            is_text = True\n",
      "            mode = \"r\"\n",
      "            if engine == \"pyarrow\":\n",
      "                is_text = False\n",
      "                mode = \"rb\"\n",
      "            elif (\n",
      "                engine == \"c\"\n",
      "                and self.options.get(\"encoding\", \"utf-8\") == \"utf-8\"\n",
      "                and isinstance(stringify_path(f), str)\n",
      "            ):\n",
      "                # c engine can decode utf-8 bytes, adding TextIOWrapper makes\n",
      "                # the c-engine especially for memory_map=True far slower\n",
      "                is_text = False\n",
      "                if \"b\" not in mode:\n",
      "                    mode += \"b\"\n",
      "            self.handles = get_handle(\n",
      "                f,\n",
      "                mode,\n",
      "                encoding=self.options.get(\"encoding\", None),\n",
      "                compression=self.options.get(\"compression\", None),\n",
      "                memory_map=self.options.get(\"memory_map\", False),\n",
      "                is_text=is_text,\n",
      "                errors=self.options.get(\"encoding_errors\", \"strict\"),\n",
      "                storage_options=self.options.get(\"storage_options\", None),\n",
      "            )\n",
      "            assert self.handles is not None\n",
      "            f = self.handles.handle\n",
      "\n",
      "        elif engine != \"python\":\n",
      "            msg = f\"Invalid file path or buffer object type: {type(f)}\"\n",
      "            raise ValueError(msg)\n",
      "\n",
      "        try:\n",
      "            return mapping[engine](f, **self.options)\n",
      "        except Exception:\n",
      "            if self.handles is not None:\n",
      "                self.handles.close()\n",
      "            raise\n",
      "\n",
      "    def _failover_to_python(self) -> None:\n",
      "        raise AbstractMethodError(self)\n",
      "\n",
      "    def read(self, nrows: int | None = None) -> DataFrame:\n",
      "        if self.engine == \"pyarrow\":\n",
      "            try:\n",
      "                # error: \"ParserBase\" has no attribute \"read\"\n",
      "                df = self._engine.read()  # type: ignore[attr-defined]\n",
      "            except Exception:\n",
      "                self.close()\n",
      "                raise\n",
      "        else:\n",
      "            nrows = validate_integer(\"nrows\", nrows)\n",
      "            try:\n",
      "                # error: \"ParserBase\" has no attribute \"read\"\n",
      "                (\n",
      "                    index,\n",
      "                    columns,\n",
      "                    col_dict,\n",
      "                ) = self._engine.read(  # type: ignore[attr-defined]\n",
      "                    nrows\n",
      "                )\n",
      "            except Exception:\n",
      "                self.close()\n",
      "                raise\n",
      "\n",
      "            if index is None:\n",
      "                if col_dict:\n",
      "                    # Any column is actually fine:\n",
      "                    new_rows = len(next(iter(col_dict.values())))\n",
      "                    index = RangeIndex(self._currow, self._currow + new_rows)\n",
      "                else:\n",
      "                    new_rows = 0\n",
      "            else:\n",
      "                new_rows = len(index)\n",
      "\n",
      "            if hasattr(self, \"orig_options\"):\n",
      "                dtype_arg = self.orig_options.get(\"dtype\", None)\n",
      "            else:\n",
      "                dtype_arg = None\n",
      "\n",
      "            if isinstance(dtype_arg, dict):\n",
      "                dtype = defaultdict(lambda: None)  # type: ignore[var-annotated]\n",
      "                dtype.update(dtype_arg)\n",
      "            elif dtype_arg is not None and pandas_dtype(dtype_arg) in (\n",
      "                np.str_,\n",
      "                np.object_,\n",
      "            ):\n",
      "                dtype = defaultdict(lambda: dtype_arg)\n",
      "            else:\n",
      "                dtype = None\n",
      "\n",
      "            if dtype is not None:\n",
      "                new_col_dict = {}\n",
      "                for k, v in col_dict.items():\n",
      "                    d = (\n",
      "                        dtype[k]\n",
      "                        if pandas_dtype(dtype[k]) in (np.str_, np.object_)\n",
      "                        else None\n",
      "                    )\n",
      "                    new_col_dict[k] = Series(v, index=index, dtype=d, copy=False)\n",
      "            else:\n",
      "                new_col_dict = col_dict\n",
      "\n",
      "            df = DataFrame(\n",
      "                new_col_dict,\n",
      "                columns=columns,\n",
      "                index=index,\n",
      "                copy=not using_copy_on_write(),\n",
      "            )\n",
      "\n",
      "            self._currow += new_rows\n",
      "        return df\n",
      "\n",
      "    def get_chunk(self, size: int | None = None) -> DataFrame:\n",
      "        if size is None:\n",
      "            size = self.chunksize\n",
      "        if self.nrows is not None:\n",
      "            if self._currow >= self.nrows:\n",
      "                raise StopIteration\n",
      "            size = min(size, self.nrows - self._currow)\n",
      "        return self.read(nrows=size)\n",
      "\n",
      "    def __enter__(self) -> Self:\n",
      "        return self\n",
      "\n",
      "    def __exit__(\n",
      "        self,\n",
      "        exc_type: type[BaseException] | None,\n",
      "        exc_value: BaseException | None,\n",
      "        traceback: TracebackType | None,\n",
      "    ) -> None:\n",
      "        self.close()\n",
      "\n",
      "\n",
      "def TextParser(*args, **kwds) -> TextFileReader:\n",
      "    \"\"\"\n",
      "    Converts lists of lists/tuples into DataFrames with proper type inference\n",
      "    and optional (e.g. string to datetime) conversion. Also enables iterating\n",
      "    lazily over chunks of large files\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    data : file-like object or list\n",
      "    delimiter : separator character to use\n",
      "    dialect : str or csv.Dialect instance, optional\n",
      "        Ignored if delimiter is longer than 1 character\n",
      "    names : sequence, default\n",
      "    header : int, default 0\n",
      "        Row to use to parse column labels. Defaults to the first row. Prior\n",
      "        rows will be discarded\n",
      "    index_col : int or list, optional\n",
      "        Column or columns to use as the (possibly hierarchical) index\n",
      "    has_index_names: bool, default False\n",
      "        True if the cols defined in index_col have an index name and are\n",
      "        not in the header.\n",
      "    na_values : scalar, str, list-like, or dict, optional\n",
      "        Additional strings to recognize as NA/NaN.\n",
      "    keep_default_na : bool, default True\n",
      "    thousands : str, optional\n",
      "        Thousands separator\n",
      "    comment : str, optional\n",
      "        Comment out remainder of line\n",
      "    parse_dates : bool, default False\n",
      "    keep_date_col : bool, default False\n",
      "    date_parser : function, optional\n",
      "\n",
      "        .. deprecated:: 2.0.0\n",
      "    date_format : str or dict of column -> format, default ``None``\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "    skiprows : list of integers\n",
      "        Row numbers to skip\n",
      "    skipfooter : int\n",
      "        Number of line at bottom of file to skip\n",
      "    converters : dict, optional\n",
      "        Dict of functions for converting values in certain columns. Keys can\n",
      "        either be integers or column labels, values are functions that take one\n",
      "        input argument, the cell (not column) content, and return the\n",
      "        transformed content.\n",
      "    encoding : str, optional\n",
      "        Encoding to use for UTF when reading/writing (ex. 'utf-8')\n",
      "    float_precision : str, optional\n",
      "        Specifies which converter the C engine should use for floating-point\n",
      "        values. The options are `None` or `high` for the ordinary converter,\n",
      "        `legacy` for the original lower precision pandas converter, and\n",
      "        `round_trip` for the round-trip converter.\n",
      "    \"\"\"\n",
      "    kwds[\"engine\"] = \"python\"\n",
      "    return TextFileReader(*args, **kwds)\n",
      "\n",
      "\n",
      "def _clean_na_values(na_values, keep_default_na: bool = True, floatify: bool = True):\n",
      "    na_fvalues: set | dict\n",
      "    if na_values is None:\n",
      "        if keep_default_na:\n",
      "            na_values = STR_NA_VALUES\n",
      "        else:\n",
      "            na_values = set()\n",
      "        na_fvalues = set()\n",
      "    elif isinstance(na_values, dict):\n",
      "        old_na_values = na_values.copy()\n",
      "        na_values = {}  # Prevent aliasing.\n",
      "\n",
      "        # Convert the values in the na_values dictionary\n",
      "        # into array-likes for further use. This is also\n",
      "        # where we append the default NaN values, provided\n",
      "        # that `keep_default_na=True`.\n",
      "        for k, v in old_na_values.items():\n",
      "            if not is_list_like(v):\n",
      "                v = [v]\n",
      "\n",
      "            if keep_default_na:\n",
      "                v = set(v) | STR_NA_VALUES\n",
      "\n",
      "            na_values[k] = v\n",
      "        na_fvalues = {k: _floatify_na_values(v) for k, v in na_values.items()}\n",
      "    else:\n",
      "        if not is_list_like(na_values):\n",
      "            na_values = [na_values]\n",
      "        na_values = _stringify_na_values(na_values, floatify)\n",
      "        if keep_default_na:\n",
      "            na_values = na_values | STR_NA_VALUES\n",
      "\n",
      "        na_fvalues = _floatify_na_values(na_values)\n",
      "\n",
      "    return na_values, na_fvalues\n",
      "\n",
      "\n",
      "def _floatify_na_values(na_values):\n",
      "    # create float versions of the na_values\n",
      "    result = set()\n",
      "    for v in na_values:\n",
      "        try:\n",
      "            v = float(v)\n",
      "            if not np.isnan(v):\n",
      "                result.add(v)\n",
      "        except (TypeError, ValueError, OverflowError):\n",
      "            pass\n",
      "    return result\n",
      "\n",
      "\n",
      "def _stringify_na_values(na_values, floatify: bool):\n",
      "    \"\"\"return a stringified and numeric for these values\"\"\"\n",
      "    result: list[str | float] = []\n",
      "    for x in na_values:\n",
      "        result.append(str(x))\n",
      "        result.append(x)\n",
      "        try:\n",
      "            v = float(x)\n",
      "\n",
      "            # we are like 999 here\n",
      "            if v == int(v):\n",
      "                v = int(v)\n",
      "                result.append(f\"{v}.0\")\n",
      "                result.append(str(v))\n",
      "\n",
      "            if floatify:\n",
      "                result.append(v)\n",
      "        except (TypeError, ValueError, OverflowError):\n",
      "            pass\n",
      "        if floatify:\n",
      "            try:\n",
      "                result.append(int(x))\n",
      "            except (TypeError, ValueError, OverflowError):\n",
      "                pass\n",
      "    return set(result)\n",
      "\n",
      "\n",
      "def _refine_defaults_read(\n",
      "    dialect: str | csv.Dialect | None,\n",
      "    delimiter: str | None | lib.NoDefault,\n",
      "    delim_whitespace: bool,\n",
      "    engine: CSVEngine | None,\n",
      "    sep: str | None | lib.NoDefault,\n",
      "    on_bad_lines: str | Callable,\n",
      "    names: Sequence[Hashable] | None | lib.NoDefault,\n",
      "    defaults: dict[str, Any],\n",
      "    dtype_backend: DtypeBackend | lib.NoDefault,\n",
      "):\n",
      "    \"\"\"Validate/refine default values of input parameters of read_csv, read_table.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    dialect : str or csv.Dialect\n",
      "        If provided, this parameter will override values (default or not) for the\n",
      "        following parameters: `delimiter`, `doublequote`, `escapechar`,\n",
      "        `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n",
      "        override values, a ParserWarning will be issued. See csv.Dialect\n",
      "        documentation for more details.\n",
      "    delimiter : str or object\n",
      "        Alias for sep.\n",
      "    delim_whitespace : bool\n",
      "        Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n",
      "        used as the sep. Equivalent to setting ``sep='\\\\s+'``. If this option\n",
      "        is set to True, nothing should be passed in for the ``delimiter``\n",
      "        parameter.\n",
      "\n",
      "        .. deprecated:: 2.2.0\n",
      "            Use ``sep=\"\\\\s+\"`` instead.\n",
      "    engine : {{'c', 'python'}}\n",
      "        Parser engine to use. The C engine is faster while the python engine is\n",
      "        currently more feature-complete.\n",
      "    sep : str or object\n",
      "        A delimiter provided by the user (str) or a sentinel value, i.e.\n",
      "        pandas._libs.lib.no_default.\n",
      "    on_bad_lines : str, callable\n",
      "        An option for handling bad lines or a sentinel value(None).\n",
      "    names : array-like, optional\n",
      "        List of column names to use. If the file contains a header row,\n",
      "        then you should explicitly pass ``header=0`` to override the column names.\n",
      "        Duplicates in this list are not allowed.\n",
      "    defaults: dict\n",
      "        Default values of input parameters.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    kwds : dict\n",
      "        Input parameters with correct values.\n",
      "\n",
      "    Raises\n",
      "    ------\n",
      "    ValueError :\n",
      "        If a delimiter was specified with ``sep`` (or ``delimiter``) and\n",
      "        ``delim_whitespace=True``.\n",
      "    \"\"\"\n",
      "    # fix types for sep, delimiter to Union(str, Any)\n",
      "    delim_default = defaults[\"delimiter\"]\n",
      "    kwds: dict[str, Any] = {}\n",
      "    # gh-23761\n",
      "    #\n",
      "    # When a dialect is passed, it overrides any of the overlapping\n",
      "    # parameters passed in directly. We don't want to warn if the\n",
      "    # default parameters were passed in (since it probably means\n",
      "    # that the user didn't pass them in explicitly in the first place).\n",
      "    #\n",
      "    # \"delimiter\" is the annoying corner case because we alias it to\n",
      "    # \"sep\" before doing comparison to the dialect values later on.\n",
      "    # Thus, we need a flag to indicate that we need to \"override\"\n",
      "    # the comparison to dialect values by checking if default values\n",
      "    # for BOTH \"delimiter\" and \"sep\" were provided.\n",
      "    if dialect is not None:\n",
      "        kwds[\"sep_override\"] = delimiter is None and (\n",
      "            sep is lib.no_default or sep == delim_default\n",
      "        )\n",
      "\n",
      "    if delimiter and (sep is not lib.no_default):\n",
      "        raise ValueError(\"Specified a sep and a delimiter; you can only specify one.\")\n",
      "\n",
      "    kwds[\"names\"] = None if names is lib.no_default else names\n",
      "\n",
      "    # Alias sep -> delimiter.\n",
      "    if delimiter is None:\n",
      "        delimiter = sep\n",
      "\n",
      "    if delim_whitespace and (delimiter is not lib.no_default):\n",
      "        raise ValueError(\n",
      "            \"Specified a delimiter with both sep and \"\n",
      "            \"delim_whitespace=True; you can only specify one.\"\n",
      "        )\n",
      "\n",
      "    if delimiter == \"\\n\":\n",
      "        raise ValueError(\n",
      "            r\"Specified \\n as separator or delimiter. This forces the python engine \"\n",
      "            \"which does not accept a line terminator. Hence it is not allowed to use \"\n",
      "            \"the line terminator as separator.\",\n",
      "        )\n",
      "\n",
      "    if delimiter is lib.no_default:\n",
      "        # assign default separator value\n",
      "        kwds[\"delimiter\"] = delim_default\n",
      "    else:\n",
      "        kwds[\"delimiter\"] = delimiter\n",
      "\n",
      "    if engine is not None:\n",
      "        kwds[\"engine_specified\"] = True\n",
      "    else:\n",
      "        kwds[\"engine\"] = \"c\"\n",
      "        kwds[\"engine_specified\"] = False\n",
      "\n",
      "    if on_bad_lines == \"error\":\n",
      "        kwds[\"on_bad_lines\"] = ParserBase.BadLineHandleMethod.ERROR\n",
      "    elif on_bad_lines == \"warn\":\n",
      "        kwds[\"on_bad_lines\"] = ParserBase.BadLineHandleMethod.WARN\n",
      "    elif on_bad_lines == \"skip\":\n",
      "        kwds[\"on_bad_lines\"] = ParserBase.BadLineHandleMethod.SKIP\n",
      "    elif callable(on_bad_lines):\n",
      "        if engine not in [\"python\", \"pyarrow\"]:\n",
      "            raise ValueError(\n",
      "                \"on_bad_line can only be a callable function \"\n",
      "                \"if engine='python' or 'pyarrow'\"\n",
      "            )\n",
      "        kwds[\"on_bad_lines\"] = on_bad_lines\n",
      "    else:\n",
      "        raise ValueError(f\"Argument {on_bad_lines} is invalid for on_bad_lines\")\n",
      "\n",
      "    check_dtype_backend(dtype_backend)\n",
      "\n",
      "    kwds[\"dtype_backend\"] = dtype_backend\n",
      "\n",
      "    return kwds\n",
      "\n",
      "\n",
      "def _extract_dialect(kwds: dict[str, Any]) -> csv.Dialect | None:\n",
      "    \"\"\"\n",
      "    Extract concrete csv dialect instance.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    csv.Dialect or None\n",
      "    \"\"\"\n",
      "    if kwds.get(\"dialect\") is None:\n",
      "        return None\n",
      "\n",
      "    dialect = kwds[\"dialect\"]\n",
      "    if dialect in csv.list_dialects():\n",
      "        dialect = csv.get_dialect(dialect)\n",
      "\n",
      "    _validate_dialect(dialect)\n",
      "\n",
      "    return dialect\n",
      "\n",
      "\n",
      "MANDATORY_DIALECT_ATTRS = (\n",
      "    \"delimiter\",\n",
      "    \"doublequote\",\n",
      "    \"escapechar\",\n",
      "    \"skipinitialspace\",\n",
      "    \"quotechar\",\n",
      "    \"quoting\",\n",
      ")\n",
      "\n",
      "\n",
      "def _validate_dialect(dialect: csv.Dialect) -> None:\n",
      "    \"\"\"\n",
      "    Validate csv dialect instance.\n",
      "\n",
      "    Raises\n",
      "    ------\n",
      "    ValueError\n",
      "        If incorrect dialect is provided.\n",
      "    \"\"\"\n",
      "    for param in MANDATORY_DIALECT_ATTRS:\n",
      "        if not hasattr(dialect, param):\n",
      "            raise ValueError(f\"Invalid dialect {dialect} provided\")\n",
      "\n",
      "\n",
      "def _merge_with_dialect_properties(\n",
      "    dialect: csv.Dialect,\n",
      "    defaults: dict[str, Any],\n",
      ") -> dict[str, Any]:\n",
      "    \"\"\"\n",
      "    Merge default kwargs in TextFileReader with dialect parameters.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    dialect : csv.Dialect\n",
      "        Concrete csv dialect. See csv.Dialect documentation for more details.\n",
      "    defaults : dict\n",
      "        Keyword arguments passed to TextFileReader.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    kwds : dict\n",
      "        Updated keyword arguments, merged with dialect parameters.\n",
      "    \"\"\"\n",
      "    kwds = defaults.copy()\n",
      "\n",
      "    for param in MANDATORY_DIALECT_ATTRS:\n",
      "        dialect_val = getattr(dialect, param)\n",
      "\n",
      "        parser_default = parser_defaults[param]\n",
      "        provided = kwds.get(param, parser_default)\n",
      "\n",
      "        # Messages for conflicting values between the dialect\n",
      "        # instance and the actual parameters provided.\n",
      "        conflict_msgs = []\n",
      "\n",
      "        # Don't warn if the default parameter was passed in,\n",
      "        # even if it conflicts with the dialect (gh-23761).\n",
      "        if provided not in (parser_default, dialect_val):\n",
      "            msg = (\n",
      "                f\"Conflicting values for '{param}': '{provided}' was \"\n",
      "                f\"provided, but the dialect specifies '{dialect_val}'. \"\n",
      "                \"Using the dialect-specified value.\"\n",
      "            )\n",
      "\n",
      "            # Annoying corner case for not warning about\n",
      "            # conflicts between dialect and delimiter parameter.\n",
      "            # Refer to the outer \"_read_\" function for more info.\n",
      "            if not (param == \"delimiter\" and kwds.pop(\"sep_override\", False)):\n",
      "                conflict_msgs.append(msg)\n",
      "\n",
      "        if conflict_msgs:\n",
      "            warnings.warn(\n",
      "                \"\\n\\n\".join(conflict_msgs), ParserWarning, stacklevel=find_stack_level()\n",
      "            )\n",
      "        kwds[param] = dialect_val\n",
      "    return kwds\n",
      "\n",
      "\n",
      "def _validate_skipfooter(kwds: dict[str, Any]) -> None:\n",
      "    \"\"\"\n",
      "    Check whether skipfooter is compatible with other kwargs in TextFileReader.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    kwds : dict\n",
      "        Keyword arguments passed to TextFileReader.\n",
      "\n",
      "    Raises\n",
      "    ------\n",
      "    ValueError\n",
      "        If skipfooter is not compatible with other parameters.\n",
      "    \"\"\"\n",
      "    if kwds.get(\"skipfooter\"):\n",
      "        if kwds.get(\"iterator\") or kwds.get(\"chunksize\"):\n",
      "            raise ValueError(\"'skipfooter' not supported for iteration\")\n",
      "        if kwds.get(\"nrows\"):\n",
      "            raise ValueError(\"'skipfooter' not supported with 'nrows'\")\n"
     ]
    }
   ],
   "source": [
    "!cat ~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990182a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3896a386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3253d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
